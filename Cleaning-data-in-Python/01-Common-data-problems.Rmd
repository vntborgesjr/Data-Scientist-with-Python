---
title: "01 -Common data problems"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

```{python setup}
import pandas as pd
import numpy as np
```

## **Data type constrains**

**1. Data type constraints**

Hi and welcome! My name is Adel, and I'll be your host as we learn how to clean data in Python.

**2. Course outline**

In this course, we're going to understand how to diagnose different problems in our data and how they can can come up during our workflow.

**3. Course outline**

We will also understand the side effects of not treating our data correctly.

**4. Course outline**

and various ways to address different types of dirty data.

**5. Course outline**

In this chapter, we're going to discuss the most common data problems you may encounter and how to address them. So let's get started!

**6. Why do we need to clean data?**

To understand why we need to clean data, let's remind ourselves of the data science workflow. In a typical data science workflow, we usually access our raw data, explore and process it, develop insights using visualizations or predictive models, and finally report these insights with dashboards or reports.

**7. Why do we need to clean data?**

Dirty data can appear because of duplicate values, mis-spellings, data type parsing errors and legacy systems.

**8. Why do we need to clean data?**

Without making sure that data is properly cleaned in the exploration and processing phase, we will surely compromise the insights and reports subsequently generated. As the old adage says, garbage in garbage out.

**9. Data type constraints**

When working with data, there are various types that we may encounter along the way. We could be working with text data, integers, decimals, dates, zip codes, and others. Luckily, Python has specific data type objects for various data types that you're probably familiar with by now. This makes it much easier to manipulate these various data types in Python. As such, before preparing to analyze and extract insights from our data, we need to make sure our variables have the correct data types, other wise we risk compromising our analysis.

**10. Strings to integers**

Let's take a look at the following example. Here's the head of a DataFrame containing revenue generated and quantity of items sold for a sales order. We want to calculate the total revenue generated by all sales orders. As you can see, the Revenue column has the dollar sign on the right hand side. A close inspection of the DataFrame column's data types using the dot-dtypes attribute returns object for the Revenue column, which is what pandas uses to store strings.

**11. String to integers**

We can also check the data types as well as the number of missing values per column in a DataFrame, by using the dot-info() method.

**12. String to integers**

Since the Revenue column is a string, summing across all sales orders returns one large concatenated string containing each row's string. To fix this, we need to first remove the $ sign from the string so that pandas is able to convert the strings into numbers without error. We do this with the dot-str-dot-strip() method, while specifying the string we want to strip as an argument, which is in this case the dollar sign. Since our dollar values do not contain decimals, we then convert the Revenue column to an integer by using the dot-astype() method, specifying the desired data type as argument. Had our revenue values been decimal, we would have converted the Revenue column to float. We can make sure that the Revenue column is now an integer by using the assert statement, which takes in a condition as input, as returns nothing if that condition is met, and an error if it is not.

**13. The assert statement**

For example, here we are testing the equality that 1+1 equals 2. Since it is the case, the assert statement returns nothing. However, when testing the equality 1+1 equals 3, we receive an assertionerror. You can test almost anything you can imagine of by using assert, and we'll see more ways to utilize it as we go along the course.

**14. Numeric or categorical?**

A common type of data seems numeric but actually represents categories with a finite set of possible categories. This is called categorical data. We will look more closely at categorical data in Chapter 2, but let's take a look at this example. Here we have a marriage status column, which is represented by 0 for never married, 1 for married, 2 for separated, and 3 for divorced. However it will be imported of type integer, which could lead to misleading results when trying to extract some statistical summaries.

**15. Numeric or categorical?**

We can solve this by using the same dot-astype() method seen earlier, but this time specifying the category data type. When applying the describe again, we see that the summary statistics are much more aligned with that of a categorical variable, discussing the number of observations, number of unique values, most frequent category instead of mean and standard deviation.

**16. Let's practice!**

Now that we have a solid understanding of data type constrains - let's get to practice!

### **Common data types**

Manipulating and analyzing data with incorrect data types could lead to compromised analysis as you go along the data science workflow.

When working with new data, you should always check the data types of your columns using the `.dtypes` attribute or the `.info()` method which you'll see in the next exercise. Often times, you'll run into columns that should be converted to different data types before starting any analysis.

In this exercise, you'll first identify different types of data and correctly map them to their respective types.

**Instructions**

- Assign each card to what type of data you think it is.

Numeric data types:

Text:

Dates:

### **Numeric data or ... ?**

In this exercise, and throughout this chapter, you'll be working with bicycle ride sharing data in San Francisco called `ride_sharing`. It contains information on the start and end stations, the trip duration, and some user information for a bike sharing service.

The `user_type` column contains information on whether a user is taking a free ride and takes on the following values:

  - `1` for free riders.
  - `2` for pay per ride.
  - `3` for monthly subscribers.
  
In this instance, you will print the information of `ride_sharing` using `.info()` and see a firsthand example of how an incorrect data type can flaw your analysis of the dataset. The `pandas` package is imported as `pd`.

**Instructions**

1. 
- Print the information of `ride_sharing`.
- Use `.describe()` to print the summary statistics of the `user_type` column from `ride_sharing`.
2.
3.

```{python}
# Print the information of ride_sharing
print(____.____())

# Print summary statistics of user_type column
print(ride_sharing['____'].____())
```

### **Summing strings and concatenating numbers**

In the previous exercise, you were able to identify that category is the correct data type for `user_type` and convert it in order to extract relevant statistical summaries that shed light on the distribution of `user_type`.

Another common data type problem is importing what should be numerical values as strings, as mathematical operations such as summing and multiplication lead to string concatenation, not numerical outputs.

In this exercise, you'll be converting the string column `duration` to the type `int`. Before that however, you will need to make sure to strip `"minutes"` from the column in order to make sure `pandas` reads it as numerical. The `pandas` package has been imported as `pd`.

**Instructions**

- Use the `.strip()` method to strip `duration` of `"minutes"` and store it in the `duration_trim` column.
- Convert `duration_trim` to `int` and store it in the `duration_time` column.
- Write an `assert` statement that checks if `duration_time`'s data type is now an `int`.
- Print the average ride duration.

```{python}
# Strip duration of minutes
ride_sharing['duration_trim'] = ride_sharing['duration'].____.____()

# Convert duration to integer
ride_sharing['duration_time'] = ____

# Write an assert statement making sure of conversion
assert ride_sharing['____'].____ == '____'

# Print formed columns and calculate average ride duration 
print(ride_sharing[['duration','duration_trim','duration_time']])
print(____)
```

## **Data range constrains**

**1. Data range constraints**

Hi and welcome back! In this lesson, we're going to discuss data that should fall within a range.

**2. Motivation**

Let's first start off with some motivation. Imagine we have a dataset of movies with their respective average rating from a streaming service. The rating can be any integer between 1 an 5.

**3. Motivation**

After creating a histogram with maptlotlib, we see that there are a few movies with an average rating of 6, which is well above the allowable range. This is most likely an error in data collection or parsing, where a variable is well beyond its range and treating it is essential to have accurate analysis.

**4. Motivation**

Here's another example, where we see subscription dates in the future for a service. Inherently this doesn't make any sense, as we cannot sign up for a service in the future, but these errors exist either due to technical or human error. We use the datetime package's dot-date-dot-today() function to get today's date, and we filter the dataset by any subscription date higher than today's date. We need to pay attention to the range of our data.

**5. How to deal with out of range data?**

There's a variety of options to deal with out of range data. The simplest option is to drop the data. However, depending on the size of your out of range data, you could be losing out on essential information. As a rule of thumb, only drop data when a small proportion of your dataset is affected by out of range values, however you really need to understand your dataset before deciding to drop values. Another option would be setting custom minimums or maximums to your columns. We could also set the data to missing, and impute it, but we'll take a look at how to deal with missing data in Chapter 3. We could also, dependent on the business assumptions behind our data, assign a custom value for any values of our data that go beyond a certain range.

**6. Movie example**

Let's take a look at the movies example mentioned earlier. We first isolate the movies with ratings higher than 5. Now if these values are affect a small set of our data, we can drop them. We can drop them in two ways - we can either create a new filtered movies DataFrame where we only keep values of avg_rating lower or equal than to 5. Or drop the values by using the drop method. The drop method takes in as argument the row indices of movies for which the avg_rating is higher than 5. We set the inplace argument to True so that values are dropped in place and we don't have to create a new column. We can make sure this is set in place using an assert statement that checks if the maximum of avg_rating is lower or equal than to 5.

**7. Movie example**

Depending on the assumptions behind our data, we can also change the out of range values to a hard limit. For example, here we're setting any value of the avg_rating column in to 5 if it goes beyond it. We can do this using the dot-loc method, which returns all cells that fit a custom row and column index. It takes as first argument the row index, or here all instances of avg_rating above 5 and as second argument the column index, which is here the avg_rating column. Again, we can make sure that this change was done using an assert statement.

**8. Date range example**

Let's take another at the date range example mentioned earlier, where we had subscriptions happening in the future. We first look at the datatypes of the column with the dot-dtypes attribute. We can confirm that the subscription_date column is an object and not a datetime object. Datetime objects allow much easier manipulation of date data, so let's convert it to that. We do so with the to_datetime function from pandas, which takes in as argument the column we want to convert. We can then test the data type conversion by asserting that the subscription date's column is equal to datetime64[ns], which is how the data type is represented in pandas.

**9. Date range example**

Now that the column is in datetime, we can treat it in a variety of ways. We first create a today_date variable using the datetime function date.today, which allows us to store today's date. We can then either drop the rows with exceeding dates similar to how we did in the average rating example, or replace exceeding values with today's date. In both cases we can use the assert statement to verify our treatment went well, by comparing the maximum value in the subscription_date column. However, make sure to chain it with the dot-date() method to return a datetime object instead of a timestamp.

**10. Let's practice!**

Now that you know all about ranges, let's practice!

### **Tire size constraints**

In this lesson, you're going to build on top of the work you've been doing with the `ride_sharing` DataFrame. You'll be working with the `tire_sizes` column which contains data on each bike's tire size.

Bicycle tire sizes could be either 26″, 27″ or 29″ and are here correctly stored as a categorical value. In an effort to cut maintenance costs, the ride sharing provider decided to set the maximum tire size to be 27″.

In this exercise, you will make sure the `tire_sizes` column has the correct range by first converting it to an integer, then setting and testing the new upper limit of 27″ for tire sizes.

**Instructions**

- Convert the `tire_sizes` column from `category` to `'int'`.
- Use `.loc[]` to set all values of `tire_sizes` above 27 to 27.
- Reconvert back `tire_sizes` to `'category'` from `int`.
- Print the description of the `tire_sizes`.

```{python}
# Convert tire_sizes to integer
ride_sharing['tire_sizes'] = ____['____'].____('____')

# Set all values above 27 to 27
ride_sharing.____[____ > ____, ____] = ____

# Reconvert tire_sizes back to categorical
ride_sharing['tire_sizes'] = ____

# Print tire size description
print(ride_sharing['tire_sizes'].____())
```

### **Back to the future**

A new update to the data pipeline feeding into the `ride_sharing` DataFrame has been updated to register each ride's date. This information is stored in the `ride_date` column of the type `object`, which represents strings in `pandas`.

A bug was discovered which was relaying rides taken today as taken next year. To fix this, you will find all instances of the `ride_date` column that occur anytime in the future, and set the maximum possible value of this column to today's date. Before doing so, you would need to convert `ride_date` to a `datetime` object.

The `datetime` package has been imported as `dt`, alongside all the packages you've been using till now.

**Instructions**

- Convert `ride_date` to a `datetime` object and store it in `ride_dt` column using `to_datetime()`.
- Create the variable `today`, which stores today's date by using the `dt.date.today()` function.
- For all instances of `ride_dt` in the future, set them to today's date.
- Print the maximum date in the `ride_dt` column.

```{python}
# Convert ride_date to datetime
ride_sharing['ride_dt'] = pd.____(____['____'])

# Save today's date
today = ____

# Set all in the future to today's date
ride_sharing.____[____['____'] > ____, '____'] = ____

# Print maximum of ride_dt column
print(ride_sharing['ride_dt'].____())
```

## **Uniqueness constraints**

**1. Uniqueness constraints**

Hi and welcome to the final lesson of this chapter. Let's discuss another common data cleaning problem, duplicate values.

**2. What are duplicate values?**

Duplicate values can be diagnosed when we have the same exact information repeated across multiple rows, for a some or all columns in our DataFrame. In this example DataFrame containing the names, address, height, and weight of individuals, the rows presented have identical values across all columns.

**3. What are duplicate values?**

In this one, there are duplicate values for all columns except the height column -- which leads us to think it's more likely a data entry error than an actual other person.

**4. Why do they happen?**

Apart from data entry and human errors alluded to in the previous slide,

**5. Why do they happen?**

duplicate data can also arise because of bugs and design errors whether in business processes or data pipelines.

**6. Why do they happen?**

However they oftenmost arise from the necessary act of joining and consolidating data from various resources, which could retain duplicate values.

**7. How to find duplicate values?**

Let's first see how to find duplicate values. In this example, we're working with a bigger version of the the height and weight data seen earlier in the video.

**8. How to find duplicate values?**

We can find duplicates in a DataFrame by using the dot-duplicated() method. It returns a Series of boolean values that are True for duplicate values, and False for non-duplicated values.

**9. How to find duplicate values?**

We can see exactly which rows are affected by using brackets as such. However, using dot-duplicated() without playing around with the arguments of the method can lead to misleading results, as all the columns are required to have duplicate values by default, with all duplicate values being marked as True except for the first occurrence. This limits our ability to properly diagnose what type of duplication we have, and how to effectively treat it.

**10. How to find duplicate rows?**

To properly calibrate how we go about finding duplicates, we will use 2 arguments from the dot-duplicated() method. The subset argument lets us set a list of column names to check for duplication. For example, it allows us to find duplicates for the first and last name columns only. The keep argument lets us keep the first occurrence of a duplicate value by setting it to the string first, the last occurrence of a duplicate value by setting it the string last, or keep all occurrences of duplicate values by setting it to False. In this example, we're checking for duplicates across the first name, last name, and address variables, and we're choosing to keep all duplicates.

**11. How to find duplicate rows?**

We see the following results -- to get a better bird's eye view of the duplicates,

**12. How to find duplicate rows?**

We sort the duplicate rows using the dot-sort_values method, choosing first_name to sort by.

**13. How to find duplicate rows?**

We find that there are four sets of duplicated rows, the first 2 being complete duplicates of each other across all columns, highlighted here in red.

**14. How to find duplicate rows?**

The other 2 being incomplete duplicates of each other highlighted here in blue with discrepancies across height and weight respectively.

**15. How to treat duplicate values?**

The complete duplicates can be treated easily. All that is required is to keep one of them only and discard the others.

**16. How to treat duplicate values?**

This can be done with the dot-drop_duplicates() method, which also takes in the same subset and keep arguments as in the dot-duplicated() method, as well as the inplace argument which drops the duplicated values directly inside the height_weight DataFrame. Here we are dropping complete duplicates only, so it's not necessary nor advisable to set a subset, and since the keep argument takes in first as default, we can keep it as such. Note that we can also set it as last, but not as False as it would keep all duplicates.

**17. How to treat duplicate values?**

This leaves us with the other 2 sets of duplicates discussed earlier, which are the same for first_name, last_name and address, but contain discrepancies in height and weight. Apart from dropping rows with really small discrepancies, we can use a statistical measure to combine each set of duplicated values.

**18. How to treat duplicate values?**

For example, we can combine these two rows into one by computing the average mean between them, or the maximum, or other statistical measures, this is highly dependent on a common sense understanding of our data, and what type of data we have.

**19. How to treat duplicate values?**

We can do this easily using the groupby method, which when chained with the agg method, lets you group by a set of common columns and return statistical values for specific columns when the aggregation is being performed. For example here, we created a dictionary called summaries, which instructs groupby to return the maximum of duplicated rows for the height column, and the mean duplicated rows for the weight column. We then group height_weight by the column names defined earlier, and chained it with the agg method, which takes in the summaries dictionary we created. We chain this entire line with the dot-reset_index() method, so that we can have numbered indices in the final output. We can verify that there are no more duplicate values by running the duplicated method again, and use brackets to output duplicate rows.

**20. Let's practice!**

Now that we have a solid grasp of duplication, let's practice.

### **How big is your subset?**

You have the following `loans` DataFrame which contains loan and credit score data for consumers, and some metadata such as their first and last names. You want to find both complete and incomplete duplicates using `.duplicated()`.

```{python}

```

Choose the **correct** usage of `.duplicated()` below:

Possible Answers

1. `loans.duplicated()` 
  Because the default method returns both complete and incomplete duplicates.

2. `loans.duplicated(subset = 'first_name')`
  Because constraining the duplicate rows to the first name lets me find incomplete duplicates as well.

3. `loans.duplicated(subset = ['first_name', 'last_name'], keep = False)`
  Because subsetting on consumer metadata and not discarding any duplicate returns all duplicated rows.

4. `loans.duplicated(subset = ['first_name', 'last_name'], keep = 'first')` 
  Because this drops all duplicates.
  
Answer:

### **Finding duplicates**

A new update to the data pipeline feeding into `ride_sharing` has added the `ride_id` column, which represents a unique identifier for each ride.

The update however coincided with radically shorter average ride duration times and irregular user birth dates set in the future. Most importantly, the number of rides taken has increased by 20% overnight, leading you to think there might be both complete and incomplete duplicates in the `ride_sharing` DataFrame.

In this exercise, you will confirm this suspicion by finding those duplicates. A sample of `ride_sharing` is in your environment, as well as all the packages you've been working with thus far.

**Instructions**

- Find duplicated rows of `ride_id` in the `ride_sharing` DataFrame while setting `keep` to `False`.
- Subset `ride_sharing` on `duplicates` and sort by `ride_id` and assign the results to `duplicated_rides`.
- Print the `ride_id`, duration and `user_birth_year` columns of `duplicated_rides` in that order.

```{python}
# Find duplicates
duplicates = ____.____(____, ____)

# Sort your duplicated rides
duplicated_rides = ride_sharing[____].____('____')

# Print relevant columns of duplicated_rides
print(duplicated_rides[['____','____','____']])
```

### **Treating duplicates**

In the last exercise, you were able to verify that the new update feeding into `ride_sharing` contains a bug generating both complete and incomplete duplicated rows for some values of the `ride_id` column, with occasional discrepant values for the `user_birth_year` and `duration` columns.

In this exercise, you will be treating those duplicated rows by first dropping complete duplicates, and then merging the incomplete duplicate rows into one while keeping the average `duration`, and the minimum `user_birth_year` for each set of incomplete duplicate rows.

**Instructions**

- Drop complete duplicates in `ride_sharing` and store the results in `ride_dup`.
- Create the `statistics` dictionary which holds minimum aggregation for `user_birth_year` and mean aggregation for `duration`.
- Drop incomplete duplicates by grouping by `ride_id` and applying the aggregation in `statistics`.
- Find duplicates again and run the `assert` statement to verify de-duplication.

```{python}
# Drop complete duplicates from ride_sharing
ride_dup = ____.____()

# Create statistics dictionary for aggregation function
statistics = {'user_birth_year': ____, 'duration': ____}

# Group by ride_id and compute new statistics
ride_unique = ride_dup.____('____').____(____).reset_index()

# Find duplicated values again
duplicates = ride_unique.____(subset = 'ride_id', keep = False)
duplicated_rides = ride_unique[duplicates == True]

# Assert duplicates are processed
assert duplicated_rides.shape[0] == 0
```

