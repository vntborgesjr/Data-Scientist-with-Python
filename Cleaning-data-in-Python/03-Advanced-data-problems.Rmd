---
title: "03 - Advanced data problems"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

```{python setup}
import pandas as pd
import numpy as np
```

## **Uniformity**

**1. Uniformity**

Stellar work on chapter 2! You're now an expert at handling categorical and text variables.

**2. In this chapter**

In this chapter, we're looking at more advanced data cleaning problems, such as uniformity, cross field validation and dealing with missing data.

**3. Data range constraints**

In chapter 1, we saw how out of range values are a common problem when cleaning data, and that when left untouched, can skew your analysis.

**4. Uniformity**

In this lesson, we're going to tackle a problem that could similarly skew our data, which is unit uniformity. For example, we can have temperature data that has values in both Fahrenheit and Celsius, weight data in Kilograms and in stones, dates in multiple formats, and so on. Verifying unit uniformity is imperative to having accurate analysis.

**5. An example**

Here's a dataset with average temperature data throughout the month of March in New York City. The dataset was collected from different sources with temperature data in Celsius and Fahrenheit merged together. We can see that unless a major climate event occurred,

**6. An example**

this value here is most likely Fahrenheit, not Celsius. Let's confirm the presence of these values visually.

**7. An example**

We can do so by plotting a scatter plot of our data. We can do this using matplotlib.pyplot, which was imported as plt. We use the plt dot scatter function, which takes in what to plot on the x axis, the y axis, and which data source to use. We set the title, axis labels with the helper functions seen here, show the plot with plt dot show,

**8. An example**
and voila.

**9. Insert title here...**
Notice these values here? They all must be fahrenheit.

**10. Treating temperature data**

A simple web search returns the formula for converting Fahrenheit to Celsius. To convert our temperature data, we isolate all rows of temperature column where it is above 40 using the loc method. We chose 40 because it's a common sense maximum for Celsius temperatures in New York City. We then convert these values to Celsius using the formula above, and reassign them to their respective Fahrenheit values in temperatures. We can make sure that our conversion was correct with an assert statement, by making sure the maximum value of temperature is less than 40.

**11. Treating date data**

Here's another common uniformity problem with date data. This is a DataFrame called birthdays containing birth dates for a variety of individuals. It has been collected from a variety of sources and merged into one.

**12. Treating date data**

Notice the dates here? The one in blue has the month, day, year format, whereas the one in orange has the month written out. The one in red is obviously an error, with what looks like a day day year format. We'll learn how to deal with that one as well.

**13. Datetime formatting**

We already discussed datetime objects. Without getting too much into detail, datetime accepts different formats that help you format your dates as pleased. The pandas to datetime function automatically accepts most date formats, but could raise errors when certain formats are unrecognizable. You don't have to memorize these formats, just know that they exist and are easily searchable!

**14. Treating date data**

You can treat these date inconsistencies easily by converting your date column to datetime. We can do this in pandas with the to_datetime function. However this isn't enough and will most likely return an error, since we have dates in multiple formats, especially the weird day/day/format which triggers an error with months. Instead we set the infer_datetime_format argument to True, and set errors equal to coerce. This will infer the format and return missing value for dates that couldn't be identified and converted instead of a value error.

**15. Treating date data**

This returns the birthday column with aligned formats, with the initial ambiguous format of day day year, being set to NAT, which represents missing values in Pandas for datetime objects.

**16. Treating date data**

We can also convert the format of a datetime column using the dt dot strftime method, which accepts a datetime format of your choice. For example, here we convert the Birthday column to day month year, instead of year month day.

**17. Treating ambiguous date data**

However a common problem is having ambiguous dates with vague formats. For example, is this date value set in March or August? Unfortunately there's no clear cut way to spot this inconsistency or to treat it. Depending on the size of the dataset and suspected ambiguities, we can either convert these dates to NAs and deal with them accordingly. If you have additional context on the source of your data, you can probably infer the format. If the majority of subsequent or previous data is of one format, you can probably infer the format as well. All in all, it is essential to properly understand where your data comes from, before trying to treat it, as it will make making these decisions much easier.

**18. Let's practice!**

Now let's make our data uniform!

### **Ambiguous dates**

You have a DataFrame containing a subscription_date column that was collected from various sources with different Date formats such as `YYYY-mm-dd` and `YYYY-dd-mm`. What is the best way to unify the formats for ambiguous values such as `2019-04-07`?

**Possible Answers**

1. Set them to `NA` and drop them.

2. Infer the format of the data in question by checking the format of subsequent and previous values.

3. Infer the format from the original data source.

4. All of the above are possible, as long as we investigate where our data comes from, and understand the dynamics affecting it before cleaning it.

### **Uniform currencies**

In this exercise and throughout this chapter, you will be working with a retail banking dataset stored in the `banking` DataFrame. The dataset contains data on the amount of money stored in accounts, their currency, amount invested, account opening date and last transaction date that were consolidated from American and European branches.

You are tasked with understanding the average account size and how investments vary by the size of account, however in order to produce this analysis accurately, you first need to unify the currency amount into dollars. The `pandas` package has been imported as `pd`, and the `banking` DataFrame is in your environment.

**Instructions**

- Find the rows of `acct_cur` in banking that are equal to `'euro'` and store them in `acct_eu`.
- Find all the rows of `acct_amount` in `banking` that fit the `acct_eu` condition, and convert them to USD by multiplying them with `1.1`.
- Find all the rows of `acct_cur` in banking that fit the `acct_eu` condition, set them to `'dollar'`.

```{python}
# Find values of acct_cur that are equal to 'euro'
acct_eu = banking['____'] == '____'

# Convert acct_amount where it is in euro to dollars
banking.loc[____, '____'] = banking.loc[____, '____'] * ____

# Unify acct_cur column by changing 'euro' values to 'dollar'
banking.loc[____, '____'] = ____

# Assert that only dollar currency remains
assert banking['acct_cur'].unique() == 'dollar'
```

### **Uniform dates**

After having unified the currencies of your different account amounts, you want to add a temporal dimension to your analysis and see how customers have been investing their money given the size of their account over each year. The `account_opened` column represents when customers opened their accounts and is a good proxy for segmenting customer activity and investment over time.

However, since this data was consolidated from multiple sources, you need to make sure that all dates are of the same format. You will do so by converting this column into a `datetime` object, while making sure that the format is inferred and potentially incorrect formats are set to missing. The `banking` DataFrame is in your environment and `pandas` was imported as `pd`.

**Instructions**

1. Print the header of `account_opened` from the `banking` DataFrame and take a look at the different results.

```{python}
# Print the header of account_opened
print(____)
```

##  **Cross field validation**

**1. Cross field validation**

Hi and welcome to the second lesson of this chapter! In this lesson we'll talk about cross field validation for diagnosing dirty data.

**2. Motivation**

Let's take a look at the following dataset. It contains flight statistics on the total number of passengers in economy, business and first class as well as the total passengers for each flight. We know that these columns have been collected and merged from different data sources, and a common challenge when merging data from different sources is data integrity, or more broadly making sure that our data is correct.

**3. Cross field validation**

This is where cross field validation comes in. Cross field validation is the use of multiple fields in your dataset to sanity check the integrity of your data. For example in our flights dataset, this could be summing economy, business and first class values and making sure they are equal to the total passengers on the plane. This could be easily done in Pandas, by first subsetting on the columns to sum, then using the sum method with the axis argument set to 1 to indicate row wise summing. We then find instances where the total passengers column is equal to the sum of the classes. And find and filter out instances of inconsistent passenger amounts by subsetting on the equality we created with brackets and the tilde symbol.

**4. Cross field validation**

Here's another example containing user IDs, birthdays and age values for a set of users. We can for example make sure that the age and birthday columns are correct by subtracting the number of years between today's date and each birthday.

**5. Cross field validation**

We can do this by first making sure the Birthday column is converted to datetime with the pandas to datetime function. We then create an object storing today's date using the datetime package's date dot today function. We then calculate the difference in years between today's date's year, and the year of each birthday by using the dot dt dot year attribute of the user's Birthday column. We then find instances where the calculated ages are equal to the actual age column in the users DataFrame. We then find and filter out the instances where we have inconsistencies using subsetting with brackets and the tilde symbol on the equality we created.

**6. What to do when we catch inconsistencies?**

So what should be the course of action in case we spot inconsistencies with cross-field validation? Just like other data cleaning problems, there is no one size fits all solution, as often the best solution requires an in depth understanding of our dataset. We can decide to either drop inconsistent data, set it to missing and impute it, or apply some rules due to domain knowledge. All these routes and assumptions can be decided upon only when you have a good understanding of where your dataset comes from and the different sources feeding into it.

**7. Let's practice!**

Now that you know about cross field validation, let's get to practice!

### **Cross field or no cross field?**

Throughout this course, you've been immersed in a variety of data cleaning problems from range constraints, data type constraints, uniformity and more.

In this lesson, you were introduced to cross field validation as a means to sanity check your data and making sure you have strong data integrity.

Now, you will map different applicable concepts and techniques to their respective categories.

**Instructions**

- Map different applicable concepts and techniques to their respective categories.

- Cross field validation:

- Not cross field validation:

### **How's our data integrity?**

New data has been merged into the `banking` DataFrame that contains details on how investments in the `inv_amount` column are allocated across four different funds A, B, C and D.

Furthermore, the age and birthdays of customers are now stored in the `age` and `birth_date` columns respectively.

You want to understand how customers of different age groups invest. However, you want to first make sure the data you're analyzing is correct. You will do so by cross field checking values of `inv_amount` and `age` against the amount invested in different funds and customers' birthdays. Both `pandas` and `datetime` have been imported as `pd` and `dt` respectively.

**Instructions**

1. 
- Find the rows where the sum of all rows of the `fund_columns` in `banking` are equal to the `inv_amount` column.
- Store the values of `banking` with consistent `inv_amount` in `consistent_inv`, and those with inconsistent ones in `inconsistent_inv`.

2.
- Store today's date into `today`, and manually calculate customers' ages and store them in `ages_manual`.
- Find all rows of `banking` where the `age` column is equal to `ages_manual` and then filter `banking` into `consistent_ages` and `inconsistent_ages`.

```{python}
# Store fund columns to sum against
fund_columns = ['fund_A', 'fund_B', 'fund_C', 'fund_D']

# Find rows where fund_columns row sum == inv_amount
inv_equ = banking[____].____(____) == ____

# Store consistent and inconsistent data
consistent_inv = ____[____]
inconsistent_inv = ____[____]

# Store consistent and inconsistent data
print("Number of inconsistent investments: ", inconsistent_inv.shape[0])
```

## **Completeness**

**1. Completeness**

Hi and welcome to the last lesson of this chapter. In this lesson, we're going to discuss completeness and missing data.

**2. What is missing data?**

Missing data is one of the most common and most important data cleaning problems. Essentially, missing data is when no data value is stored for a variable in an observation. Missing data is most commonly represented as NA or NaN, but can take on arbitrary values like 0 or dot. Like a lot of the problems that we've seen thus far in the course, it's commonly due to technical or human errors. Missing data can take many forms, so let's take a look at an example.

**3. Airquality example**

Let's take a look at the airquality dataset. It contains temperature and CO2 measurements for different dates.

**4. Airquality example**

We can see that the CO2 value in this row is represented as NaN

**5. Airquality example**

We can find rows with missing values by using the dot is na method, which returns True for missing values and False for complete values across all our rows and columns.

**6. Airquality example**

We can also chain the isna method with the sum method, which returns a breakdown of missing values per column in our dataframe. We notice that the CO2 column is the only column with missing values - let's find out why and dig further into the nature of this missingness by first visualizing our missing values.

**7. Missingno**

The missingno package allows to create useful visualizations of our missing data. Digging into its details is not part of the course, but you can also check out other courses on missing data in DataCamp's course library. We visualize the missingness of the airquality DataFrame with the msno dot matrix function, and show it with pyplot's show function from matplotlib, which returns

**8. Missingno**

the following image. This matrix essentially shows how missing values are distributed across a column. We see that missing CO2 values are randomly scattered throughout the column, but is that really the case? Let's dig deeper.

**9. Airquality example**

We first isolate the rows of airquality with missing CO2 values in one DataFrame, and complete CO2 values in another.

**10. Airquality example**

Then, let's use the describe method on each of the created DataFrames.

**11. Airquality example**

We see that for all missing values of CO2, they occur at really low temperatures, with the mean temperature at minus 39 degrees and a minimum and maximum of -49 and -30 respectively. Let's confirm this visually with the missngno package.

**12. Airquality example**

We first sort the DataFrame by the temperature column. Then we input the sorted dataframe to the matrix function from msno. This leaves us with this matrix.

**13. Airquality example**

Notice how all missing values are on the top? This is because values are sorted from smallest to largest by default. This essentially confirms that CO2 measurements are lost for really low temperatures. Must be a sensor failure!

**14. Missingness types**

This leads us to missingness types. Without going too much into the details, there are a variety of types of missing data. It could missing completely at random, missing at random, or missing not at random.

**15. Missingness types**

Missing completely at random data is when there missing data completely due to randomness, and there is no relationship between missing data and remaining values, such data entry errors.

**16. Missingness types**

Despite a slightly deceiving name, Missing at random data is when there is a relationship between missing data and other observed values, such as our CO2 data being missing for low temperatures.

**17. Missingness types**

When data is missing not at random, there is a systematic relationship between the missing data and unobserved values. For example, when it's really hot outside, the thermometer might stop working, so we don't have temperature measurements for days with high temperatures. However, we have no way to tell this just from looking at the data since we can't actually see what the missing temperatures are.

**18. How to deal with missing data?**

There's a variety of ways of dealing with missing data, from dropping missing data, to imputing them with statistical measures such as mean, median or mode, or imputing them with more complicated algorithmic approaches or ones that require some machine learning. Each missingness type requires a specific approach, and each type of approach has drawbacks and positives, so make sure to dig deeper in DataCamp's course library on dealing with missing data.

**19. Dealing with missing data**

In this lesson, we'll just explore the simple approaches to dealing with missing data. Let's grab another look at the header of airquality.

**20. Dropping missing values**

We can drop missing values, by using the dot dropna method, alongside the subset argument which lets us pick which column's missing values to drop.

**21. Replacing with statistical measures**

We can also replace the missing values of CO2 with the mean value of CO2, by using the fillna method, which is in this case 1.73. Fillna takes in a dictionary with columns as keys, and the imputed value as values. We can even feed custom values into fillna pertaining to our missing data if we have enough domain knowledge about our dataset.

**22. Let's practice!**

Now that you know how to tackle missing data, let's get started!

### **Is this missing at random?**

You've seen in the video exercise how there are a variety of missingness types when observing missing data. As a reminder, missingness types can be described as the following:

  - **_Missing Completely at Random_**: _No systematic relationship between a column's missing values and other or own values_.
  - **_Missing at Random_**: *There is a systematic relationship between a column's missing values and other observed values*.
  - **_Missing not at Random_**: *There is a systematic relationship between a column's missing values and unobserved values*.

You have a DataFrame containing customer satisfaction scores for a service. What type of missingness is the following? 

                              *A customer `satisfaction_score` column with missing values for highly dissatisfied customers.*

**Possible Answers**

1. Missing completely at random.

2. Missing at random.

3. Missing not at random.

Answer:

### **Missing investors**

Dealing with missing data is one of the most common tasks in data science. There are a variety of types of missingness, as well as a variety of types of solutions to missing data.

You just received a new version of the `banking` DataFrame containing data on the amount held and invested for new and existing customers. However, there are rows with missing `inv_amount` values.

You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. The `pandas`, `missingno` and `matplotlib.pyplot` packages have been imported as `pd`, `msno` and `plt` respectively. The `banking` DataFrame is in your environment.

**Instructions**

1.
- Print the number of missing values by column in the `banking` DataFrame.
- Plot and show the missingness matrix of `banking` with the `msno.matrix()` function.

```{python}
# Print number of missing values in banking
print(____)

# Visualize missingness matrix
____
____
```

### **Follow the money**

In this exercise, you're working with another version of the `banking` DataFrame that contains missing values for both the `cust_id` column and the `acct_amount` column.

You want to produce analysis on how many unique customers the bank has, the average amount held by customers and more. You know that rows with missing `cust_id` don't really help you, and that on average `acct_amount` is usually 5 times the amount of `inv_amount`.

In this exercise, you will drop rows of `banking` with missing `cust_ids`, and impute missing values of `acct_amount` with some domain knowledge.

**Instructions**

- Use `.dropna()` to drop missing values of the `cust_id` column in `banking` and store the results in `banking_fullid`.
- Compute the estimated `acct_amount` of `banking_fullid` knowing that `acct_amount` is usually `inv_amount * 5` and assign the results to `acct_imp`.
- Impute the missing values of `acct_amount` in `banking_fullid` with the newly created `acct_imp` using `.fillna()`.

```{python}
# Drop missing values of cust_id
banking_fullid = banking.____(subset = ['____'])

# Compute estimated acct_amount
acct_imp = ____

# Impute missing acct_amount with corresponding acct_imp
banking_imputed = banking_fullid.____({'____':____})

# Print number of missing values
print(banking_imputed.isna().sum())
```

