---
title: "01 -Introduction and flat files"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
    code_folding: hide
---

```{python setup}
import pandas as pd
import numpy as np
```

## **Welcome to the course**

**1. Welcome to the course!**

Welcome to the first course on Importing Data in Python! My name is Hugo Bowne-Anderson and I am a Data Scientist at DataCamp.

**2. Import data**

In this course, you'll learn how to import data from a large variety of import data sources, for example, (i) flat files such as dot txts and dot csvs; (ii) files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files;

**3. Import data**

(iii) relational databases such as SQLite & PostgreSQL. We’ll cover all of these topics in this course.

**4. Plain text files**

First off, we're going to learn how to import basic text files, which we can broadly classify into 2 types of files - those containing plain text, such as the opening of Mark Twain's novel The Adventures of Huckleberry Finn, which you can see here,

**5. Table data**

and those containing records, that is, table data, such as titanic dot csv, in which each

1 Source: Kaggle

**6. Table data**

row is a unique passenger onboard and each

**7. Table data**

column is a characteristic or feature, such as gender, cabin and 'survived or not'. The latter is known as a flat file and we'll come back to these in a minute.

**8. Reading a text file**

In this section, we'll figure out how to read lines from a plain text file: So let's do it! To check out any plain text file, you can use Python’s basic open function to open a connection to the file. To do so, you assign the filename to a variable as a string, pass the filename to the function open and also pass it the argument mode equals 'r', which makes sure that we can only read it (we wouldn't want to accidentally write to it!), assign the text from the file to a variable text by applying the method read to the connection to the file. After you do this, make sure that you close the connection to the file using the command file dot close. It’s always best practice to clean while cooking!

```{python}
filename = "/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/huck_finn.txt"
file = open(filename, mode = 'r') # r is to read
text = file.read()
file.close()
```

**9. Printing a text file**

You can then print the file to console and check it out using the command print(text). A brief side note:

```{python}
print(text)
```

**10. Writing to a file**

if you wanted to open a file in order to write to it, you would pass it the argument mode equals 'w'. We won't use that in this course as this is course on Importing Data but it is good to know. You can avoid having to close the connection to the file by

```{python}
filename = "/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/huck_finn.txt"
file = open(filename, mode = 'w') # w is to write
file.close()
```

**11. Context manager with**

using a with statement. This allows you to create a context in which you can execute commands with the file open. Once out of this clause/context, the file is no longer open and, for this reason, with is called a Context Manager. What you're doing here is called 'binding' a variable in the context manager construct; while still within this construct, the variable file will be bound to open(filename, 'r'). It is best practice to use the with statement as you never have to concern yourself with closing the files again.

```{python}
filename = "/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/huck_finn.txt"
with open(filename, mode = 'r') as file: # r is to read
  print(file.read())
```

**12. In the exercises, you’ll:**

In the following interactive coding sessions, you’ll figure out how to print files to console. You’ll also learn to print specific lines, which can be very useful for large files. Then we’ll be back to discuss flat files and then I'll show you how to use the Python package NumPy to make our job of importing flat files & numerical data a far easier beast to tame.

**13. Let's practice!**

Enjoy!

### **Exploring your working directory**

In order to import data into Python, you should first have an idea of what files are in your working directory.

IPython, which is running on DataCamp's servers, has a bunch of cool commands, including its [magic commands](http://ipython.readthedocs.io/en/stable/overview.html). For example, starting a line with `!` gives you complete system shell access. This means that the IPython magic command `! ls` will display the contents of your current directory. Your task is to use the IPython magic command `! ls` to check out the contents of your current directory and answer the following question: which of the following files is in your working directory?

**Possible Answers**

1. huck_finn.txt

2. titanic.csv

3. moby_dick.txt

Answer: 3

### **Importing entire text files**

In this exercise, you'll be working with the file `moby_dick.txt`. It is a text file that contains the opening sentences of Moby Dick, one of the great American novels! Here you'll get experience opening a text file, printing its contents to the shell and, finally, closing it.

**Instructions**

- Open the file `moby_dick.txt` as read-only and store it in the variable `file`. Make sure to pass the filename enclosed in quotation marks `''`.
- Print the contents of the file to the shell using the `print()` function. As Hugo showed in the video, you'll need to apply the method `read()` to the object `file`.
- Check whether the file is closed by executing `print(file.closed)`.
- Close the file using the `close()` method.
- Check again that the file is closed as you did above.

```{python}
# Open a file: file
file = open('/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/moby_dick.txt', mode = 'r')

# Print it
print(file.read())

# Check whether file is closed
print(file.closed)

# Close file
file.close()

# Check whether file is closed
print(file.closed)

```

### **Importing text files line by line**

For large files, we may not want to print all of their content to the shell: you may wish to print only the first few lines. Enter the `readline()` method, which allows you to do this. When a file called `file` is open, you can print out the first line by executing `file.readline()`. If you execute the same command again, the second line will print, and so on.

In the introductory video, Hugo also introduced the concept of a **context manager**. He showed that you can bind a variable `file` by using a context manager construct:

`with open('huck_finn.txt') as file:`

While still within this construct, the variable file will be bound to `open('huck_finn.txt')`; thus, to print the file to the shell, all the code you need to execute is:

`with open('huck_finn.txt') as file:
    print(file.readline())`

You'll now use these tools to print the first few lines of `moby_dick.txt`!

**Instructions**

- Open `moby_dick.txt` using the `with` context manager and the variable `file`.
- Print the first three lines of the file to the shell by using `readline()` three times within the context manager.

```{python}
# Read & print the first 3 lines
with open('/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/moby_dick.txt') as file:
    print(file.readline())
    print(file.readline())
    print(file.readline())
quit()
```

## **The importance of flat files in data science**

**1. The importance of flat files in data science**

Now you know how to import plain text files,

**2. Flat files**

we're going to look at flat files, such as 'titanic dot csv',

**3. Flat files**

in which each

**4. Flat files**

row is a unique passenger onboard and each

**5. Flat files**

column is a feature of attribute, such as gender, cabin and 'survived or not'. It is essential for any budding data scientist to know precisely what the term flat file means.

**6. Flat files**

Flat files are basic text files containing records, that is, table data, without structured relationships. This is in contrast to a relational database, for example, in which columns of distinct tables can be related. We'll get to these later. To be even more precise, flat files consist of records, where by a record we mean a row of fields or attributes, each of which contains at most one item of information. In the flat file 'titanic dot csv', each

**7. Flat files**

row or record is a unique passenger onboard and each column is a feature or attribute, such as

**8. Flat files**

name, gender and cabin.

**9. Header**

It is also essential to note that a flat file can have a header, such as in 'titanic dot csv', which is a

**10. Header**

row that occurs as the first row and describes the contents of the data columns or states what the corresponding attributes or features in each column are. It will be important to know whether or not your file has a header as it may alter your data import. The reason that flat files are so important in data science is that we data scientists really honestly like to think in records or rows of attributes.

**11. File extension**

Now you may have noticed that the file extension was dot csv. You may be wondering what this is? Well, CSV is an acronym for comma separated value and it means exactly what it says. The values in each row are separated by commas. Another common extension for a flat file is dot txt, which means a text file. Values in flat files can be separated by characters or sequences of characters other than commas, such as a tab, and the character or characters in question is called a delimiter.

**12. Tab-delimited file**

See here an example of a tab-delimited file. The data consists of the famous MNIST digit recognition images, where

**13. Tab-delimited file**

each row contains the pixel values of a given image. Note that all fields in the MNIST data are numeric, while the 'titanic dot csv' also contained strings.

**14. How do you import flat files?**

How do we import such files? If they consist entirely of numbers and we want to store them as a numpy array, we could use numpy. If, instead, we want to store the data in a dataframe, we could use pandas. Most of the time, you will use one of these options. In the rest of this Chapter, you'll learn how to import flat files that contain only numerical data, such as the MNIST data, and import flat files that contain both numerical data and strings, such as 'titanic dot csv'.

**15. Let's practice!**

But first, lets get you to do a couple of quick multiple choice questions to test your knowledge of flat files.

### **Pop quiz: examples of flat files**

You're now well-versed in importing text files and you're about to become a wiz at importing flat files. But can you remember exactly what a flat file is? Test your knowledge by answering the following question: which of these file types below is NOT an example of a flat file?

**Possible Answers**

1. A .csv file.

2. A tab-delimited .txt.

3. A relational database (e.g. PostgreSQL).

Answer:

## **Pop quiz: what exactly are flat files?**

Which of the following statements about flat files is incorrect?

**Possible Answers**

1. Flat files consist of rows and each row is called a record.

2. Flat files consist of multiple tables with structured relationships between the tables.

3. A record in a flat file is composed of *fields* or *attributes*, each of which contains at most one item of information.

4. Flat files are pervasive in data science.

### **Why we like flat files and the Zen of Python**

In PythonLand, there are currently hundreds of Python Enhancement Proposals, commonly referred to as *PEP*s. [PEP8](https://www.python.org/dev/peps/pep-0008/), for example, is a standard style guide for Python, written by our sensei Guido van Rossum himself. It is the basis for how we here at DataCamp ask our instructors to [style their code](https://www.datacamp.com/teach/documentation#tab_style_guide_python). Another one of my favorites is [PEP20](https://www.python.org/dev/peps/pep-0020/), commonly called the *Zen of Python*. Its abstract is as follows:

```Long time Pythoneer Tim Peters succinctly channels the BDFL's guiding principles for Python's design into 20 aphorisms, only 19 of which have been written down.```

If you don't know what the acronym `BDFL` stands for, I suggest that you look [here](https://docs.python.org/3.3/glossary.html#term-bdfl). You can print the Zen of Python in your shell by typing `import this` into it! You're going to do this now and the 5th aphorism (line) will say something of particular interest.

The question you need to answer is: **what is the 5th aphorism of the Zen of Python?**

**Possible Answers**

1. Flat is better than nested.

2. Flat files are essential for data science.

3. The world is representable as a flat file.

4. Flatness is in the eye of the beholder.

## **Import flat files using NumPy**

**1. Importing flat files using NumPy**

Okay so you now know how to use Python’s built-in open function to open text files. What if you now want to import a flat file and assign it to a variable? If all the data are numerical, you can use the package numpy to import the data as a numpy array. Why would we want to do this?

**2. Why NumPy?**

First off, numpy arrays are the Python standard for storing numerical data. They are efficient, fast and clean.

**3. Why NumPy?**

Secondly, numpy arrays are often essential for other packages, such as scikit-learn, a popular Machine Learning package for Python. Numpy itself has a number of built-in functions that make it far easier and more efficient for us to import data as arrays. Enter the NumPy functions loadtxt and genfromtxt.

**4. Importing flat files using NumPy**

To use either of these we first need to import NumPy. We then call loadtxt and pass it the filename as the first argument, along with the delimiter as the 2nd argument. Note that the default delimiter is any white space so we’ll usually need to specify it explicitly.

**5. Customizing your NumPy import**

There are a number of additional arguments you may wish to specify. If, for example, your data consists of numerics and your header has strings in it, such as in the MNIST digits data, you will want to skip the first row by calling loadtxt with the argument skiprows equals 1; if you want only the 1st and 3rd columns of the data,

**6. Customizing your NumPy import**

you’ll want to set usecols equals the list containing ints 0 and 2.

**7. Customizing your NumPy import**

You can also import different datatypes into NumPy arrays: for example, setting the argument dtype equals 'str' will ensure that all entries are imported as strings. loadtxt is great for basic cases, but tends to break down when we have

**8. Mixed datatypes**

mixed datatypes, for example,

1 Source: Kaggle

**9. Mixed datatypes**

columns consisting of floats AND columns consisting of strings, such as we saw in the Titanic dataset.

1 Source: Kaggle

**10. Let's practice!**

Now it's your turn to have fun with loadtxt. You'll also gain hands-on experience with other functions that can handle mixed datatypes. In the next video we’ll see that, although NumPy arrays can handle data of mixed types, the natural place for such data really is the dataframe.

### **Using NumPy to import flat files**

In this exercise, you're now going to load the MNIST digit recognition dataset using the numpy function `loadtxt()` and see just how easy it can be:

  - The first argument will be the filename.
  - The second will be the delimiter which, in this case, is a comma.

You can find more information about the MNIST dataset [here](http://yann.lecun.com/exdb/mnist/) on the webpage of Yann LeCun, who is currently Director of AI Research at Facebook and Founding Director of the NYU Center for Data Science, among many other things.

**Instructions**

- Fill in the arguments of `np.loadtxt()` by passing `file` and a comma `','` for the delimiter.
- Fill in the argument of `print()` to print the type of the object `digits`. Use the function `type()`.
- Execute the rest of the code to visualize one of the rows of the data.

```{python}
# Import package
import numpy as np

# Assign filename to variable: file
file = 'digits.csv'

# Load file as array: digits
digits = np.loadtxt(____, delimiter=____)

# Print datatype of digits
print(____)

# Select and reshape a row
im = digits[21, 1:]
im_sq = np.reshape(im, (28, 28))

# Plot reshaped data (matplotlib.pyplot already loaded as plt)
plt.imshow(im_sq, cmap='Greys', interpolation='nearest')
plt.show()

```

### **Customizing your NumPy import**

What if there are rows, such as a header, that you don't want to import? What if your file has a delimiter other than a comma? What if you only wish to import particular columns?

There are a number of arguments that `np.loadtxt()` takes that you'll find useful:

  - delimiter changes the delimiter that `loadtxt()` is expecting.
      - You can use `','` for comma-delimited.
      - You can use `'\t'` for tab-delimited.
  - `skiprows` allows you to specify how many rows (not indices) you wish to skip
  - `usecols` takes a list of the indices of the columns you wish to keep.

The file that you'll be importing, `digits_header.txt`, has a header and is tab-delimited.

**Instructions**

- Complete the arguments of `np.loadtxt()`: the file you're importing is tab-delimited, you want to skip the first row and you only want to import the first and third columns.
- Complete the argument of the `print()` call in order to print the entire array that you just imported.


```{python}
# Import numpy
import numpy as np

# Assign the filename: file
file = 'digits_header.txt'

# Load the data: data
data = np.loadtxt(____, delimiter=____, skiprows=____, usecols=____)

# Print data
print(____)

```

### **Importing different datatypes**

The file `seaslug.txt`

  - has a text header, consisting of strings
  - is tab-delimited.
  
These data consists of percentage of sea slug larvae that had metamorphosed in a given time period. Read more [here](http://www.stat.ucla.edu/~rgould/datasets/aboutseaslugs.html).

Due to the header, if you tried to import it as-is using `np.loadtxt()`, Python would throw you a `ValueError` and tell you that it `could not convert string to float`. There are two ways to deal with this: firstly, you can set the data type argument `dtype` equal to `str` (for string).

Alternatively, you can skip the first row as we have seen before, using the `skiprows` argument.

**Instructions**

- Complete the first call to `np.loadtxt()` by passing `file` as the first argument.
- Execute `print(data[0])` to print the first element of `data.`
- Complete the second call to `np.loadtxt()`. The file you're importing is tab-delimited, the datatype is `float`, and you want to skip the first row.
- Print the 10th element of `data_float` by completing the `print()` command. Be guided by the previous `print()` call.
- Execute the rest of the code to visualize the data.

```{python}
# Assign filename: file
file = 'seaslug.txt'

# Import file: data
data = np.loadtxt(____, delimiter='\t', dtype=str)

# Print the first element of data
print(data[0])

# Import data as floats and skip the first row: data_float
data_float = np.loadtxt(____, delimiter=____, dtype=____, skiprows=____)

# Print the 10th element of data_float
print(____)

# Plot a scatterplot of the data
plt.scatter(data_float[:, 0], data_float[:, 1])
plt.xlabel('time (min.)')
plt.ylabel('percentage of larvae')
plt.show()

```

### **Working with mixed datatypes (1)**

Much of the time you will need to import datasets which have different datatypes in different columns; one column may contain strings and another floats, for example. The function `np.loadtxt()` will freak at this. There is another function, `np.genfromtxt()`, which can handle such structures. If we pass `dtype=None` to it, it will figure out what types each column should be.

Import `'titanic.csv'` using the function `np.genfromtxt()` as follows:

`data = np.genfromtxt('titanic.csv', delimiter=',', names=True, dtype=None)`

Here, the first argument is the filename, the second specifies the delimiter `,` and the third argument `names` tells us there is a header. Because the data are of different types, `data` is an object called a [structured array](http://docs.scipy.org/doc/numpy/user/basics.rec.html). Because numpy arrays have to contain elements that are all the same type, the structured array solves this by being a 1D array, where each element of the array is a row of the flat file imported. You can test this by checking out the array's shape in the shell by executing `np.shape(data)`.

Accessing rows and columns of structured arrays is super-intuitive: to get the ith row, merely execute `data[i]` and to get the column with name `'Fare'`, execute `data['Fare']`.

After importing the Titanic data as a structured array (as per the instructions above), print the entire column with the name `Survived` to the shell. What are the last 4 values of this column?

**Possible Answers**

1. 1,0,0,1.

2. 1,2,0,0.

3. 1,0,1,0.

4. 0,1,1,1.

Answer: 

### **Working with mixed datatypes (2)**

You have just used `np.genfromtxt()` to import data containing mixed datatypes. There is also another function `np.recfromcsv()` that behaves similarly to `np.genfromtxt()`, except that its default `dtype` is `None`. In this exercise, you'll practice using this to achieve the same result.

**Instructions**

- Import `titanic.csv` using the function `np.recfromcsv()` and assign it to the variable, `d`. You'll only need to pass file to it because it has the defaults `delimiter=','` and `names=True` in addition to `dtype=None`!
- Run the remaining code to print the first three entries of the resulting array `d`.

```{python}
# Assign the filename: file
file = 'titanic.csv'

# Import file using np.recfromcsv: d


# Print out first three entries of d
print(d[:3])

```

## **Importing flat files using pandas**

**1. Importing flat files using pandas**

Congrats! You're now able to import a bunch of different types of flat files into Python as NumPy arrays. Although arrays are incredibly powerful and serve a number of essential purposes, they cannot fulfill one of the most basic needs of a Data Scientist:

**2. What a data scientist needs**

to have "[two]-dimensional labeled data structure[s] with columns of potentially different types" that you can easily perform a plethora of Data Sciencey type things on: manipulate, slice, reshaped, groupby, join, merge, perform statistics in a missing-value-friendly manner, deal with times series. The need for such a data structure, among other issues,

**3. Pandas and the DataFrame**

prompted Wes McKinney to develop the

**4. Pandas and the DataFrame**

pandas library for Python. Nothing speaks to the project of pandas more than the documentation itself:

**5. Pandas and the DataFrame**

"Python has long been great for data munging and preparation, but less so for data analysis and modeling. pandas helps fill this gap, enabling you to carry out your entire data analysis workflow in Python without having to switch to a more domain specific language like R." The data structure most relevant to the data manipulation and analysis workflow that pandas offers is the dataframe and it is the Pythonic analogue of R’s dataframe.

**6. Pandas and the DataFrame**

As Hadley Wickham tweeted, "A matrix has rows and columns. A data frame has observations and variables."

**7. Manipulating pandas DataFrames**

Manipulating dataframes in pandas can be useful in all steps of the data scientific method, from exploratory data analysis to data wrangling, preprocessing, building models and visualization. Here we will see its great utility in importing flat files, even merely in the way that it deals with missing data, comments along with the many other issues that plague working data scientists. For all of these reasons, it is now standard and best practice in Data Science to use pandas to import flat files as dataframes. Later in this course, we’ll see how many other types of data, whether they’re stored in relational databases, hdf5, MATLAB or excel files, can easily be imported as dataframes.

**8. Importing using pandas**

To use pandas, you first need to import it. Then, if we wish to import a CSV in the most basic case all we need to do is to call the function read_csv and supply it with a single argument, the name of the file. Having assigned the dataframe to the variable data, we can check the first 5 rows of the dataframe, including the header, with the command 'data dot head'. We can also easily convert to the dataframe to a numpy array by calling the dataframe attribute values. Now it's your turn to play around importing flat files using Python.

**9. You’ll experience:**

You'll get experience importing a flat file that is straightforward and you'll also get experience importing a flat file that has a few issues, such as containing comments and strings that should be interpreted as missing values.

**10. Let's practice!**

Have fun importing!

### **Using pandas to import flat files as DataFrames (1)**

In the last exercise, you were able to import flat files containing columns with different datatypes as `numpy` arrays. However, the `DataFrame` object in pandas is a more appropriate structure in which to store such data and, thankfully, we can easily import files of mixed data types as DataFrames using the pandas functions `read_csv()` and `read_table()`.

**Instructions**

- Import the `pandas` package using the alias `pd`.
- Read `titanic.csv` into a DataFrame called `df`. The file name is already stored in the `file` object.
- In a `print()` call, view the head of the DataFrame.

```{python}
# Import pandas as pd


# Assign the filename: file
file = 'titanic.csv'

# Read the file into a DataFrame: df
df = pd.read_csv(____)

# View the head of the DataFrame


```

### **Using pandas to import flat files as DataFrames (2)**

In the last exercise, you were able to import flat files into a `pandas` DataFrame. As a bonus, it is then straightforward to retrieve the corresponding `numpy` array using the attribute `values`. You'll now have a chance to do this using the MNIST dataset, which is available as `digits.csv`.

**Instructions**

- Import the first 5 rows of the file into a DataFrame using the function `pd.read_csv()` and assign the result to `data`. You'll need to use the arguments `nrows` and `header` (there is no header in this file).
- Build a numpy array from the resulting DataFrame in `data`  and assign to `data_array`.
- Execute `print(type(data_array))` to print the datatype of `data_array`.

```{python}
# Assign the filename: file
file = 'digits.csv'

# Read the first 5 rows of the file into a DataFrame: data


# Build a numpy array from the DataFrame: data_array


# Print the datatype of data_array to the shell
print(type(data_array))
```

### **Customizing your pandas import**

The `pandas` package is also great at dealing with many of the issues you will encounter when importing data as a data scientist, such as comments occurring in flat files, empty lines and missing values. Note that missing values are also commonly referred to as `NA` or `NaN`. To wrap up this chapter, you're now going to import a slightly corrupted copy of the Titanic dataset `titanic_corrupt.txt`, which

  - contains comments after the character `'#'`
  - is tab-delimited.
  
**Instructions**

- Complete the `sep` (the `pandas` version of `delim`), `comment` and `na_values` arguments of `pd.read_csv()`. `comment` takes characters that comments occur after in the file, which in this case is `'#'`. `na_values` takes a list of strings to recognize as `NA`/`NaN`, in this case the string `'Nothing'`.
- Execute the rest of the code to print the head of the resulting DataFrame and plot the histogram of the `'Age'` of passengers aboard the Titanic.

```{python}
# Import matplotlib.pyplot as plt
import matplotlib.pyplot as plt

# Assign filename: file
file = 'titanic_corrupt.txt'

# Import file: data
data = pd.read_csv(file, sep=____, comment=____, na_values=____)

# Print the head of the DataFrame
print(data.head())

# Plot 'Age' variable in a histogram
pd.DataFrame.hist(data[['Age']])
plt.xlabel('Age (years)')
plt.ylabel('count')
plt.show()

```

## **Final thougths on data importing**

**1. Final thoughts on data import**

We have seen a number of ways to read, print and import flat files. As a data scientist, you will most often wish to use pandas, however it was important to check out all the possible ways to import because you never know when they will be useful.

**2. Next chapters:**

In the next chapter, we'll see just how useful pandas can be when attempting to import a variety of other file types, such as Excel spreadsheets, along with native SAS & Stata files. It is also important to remember that, due to the active development community in open source softwares, there is constant activity in file formats and ways to import data: for example, on March 29, 2016, Wes McKinney, the creator of pandas, and Hadley Wickham, of R development fame, announced a new and fast on-disk format for dataframes for R and Python, called feather . As dataframes are one the most important data structures for data scientists, let's definitely keep our eyes on feather. After learning to import many other file types in the next chapter, you'll learn how to interact with relational databases in Python.

**3. Next course:**

Then, in the sequel to this course, you'll learn how to tear all types of data down from the web and how to interact with APIs to fulfil your big data fix. These are all essential techniques for the modern day Data Scientist to master, and the upcoming Chapters will place you in good stead to becoming

**4. Let's practice!**

an Importing Data ninja Pythonista.