---
title: "03 - Introduction to hyposthesis testing"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
    code_folding: hide
    df_print: kable
---

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Set default Seaborn style
sns.set()

election = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/2008_election_results_swing.csv")
election['total'] = election.total_votes/900
total = election.total_votes.values/900
dem_share = election.dem_share.values
dem_share_PA = election.dem_share.values[(election.state == 'PA')]
dem_share_OH = election.dem_share.values[(election.state == 'OH')]

sheffield_weather_station = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/sheffield_weather_station.csv")
rain_june = sheffield_weather_station.rain.values[(sheffield_weather_station.mm == 6)]
rain_november = sheffield_weather_station.rain.values[(sheffield_weather_station.mm == 11)]

frogs = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/frog_tongue_data.csv")
df = frogs[frogs.ID.isin(['II', 'IV'])]
df = df[['ID', 'impact force (mN)']]
df.columns = ['ID', 'impact_force']
df = df.replace(to_replace = ['II', 'IV'], value = ['A', 'B'], regex = True)
df.impact_force = df.impact_force/1000
force_a = df.impact_force[df.ID == 'A'].values
force_b = df.impact_force[df.ID == 'B'].values
forces_concat = np.concatenate([force_a, force_b])

light = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/speed-of-light.csv")
michelson_speed_of_light = np.array(light['velocity of light in air (km/s)'])

def ecdf(data):
  """Compute ECDF for a one-dimensional array of measurements."""
  # Number of data points: n
  n = len(data)
  # x-data for the ECDF: x
  x = np.sort(data)
  # y-data for the ECDF: y
  y = np.arange(1, n + 1) / n
  return x, y

def bootstrap_replicate_1d(data, func):
  """Generate bootstrap replicate of 1d data"""
  bs_sample = np.random.choice(data, len(data))
  return func(bs_sample)

def draw_bs_reps(data, func, size = 1):
  """Draw bootstrap replicates."""
  # Initialize array of replicates: bs_replicates
  bs_replicates = np.empty(size)
  # Generate replicates
  for i in range(size):
    bs_replicates[i] = bootstrap_replicate_1d(data, func)
  return bs_replicates

def draw_bs_pairs_linreg(x, y, size=1):
  """Perform pairs bootstrap for linear regression."""
  # Set up array of indices to sample from: inds
  inds = np.arange(len(x))
  # Initialize replicates: bs_slope_reps, bs_intercept_reps
  bs_slope_reps = np.empty(size)
  bs_intercept_reps = np.empty(size)
  # Generate replicates
  for i in range(size):
    bs_inds = np.random.choice(inds, size = len(inds))
    bs_x, bs_y = x[bs_inds], y[bs_inds]
    bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
  return bs_slope_reps, bs_intercept_reps
```

## **Formulating and simulating a hypothesis**

**1. Formulating and simulating a hypothesis**

When we studied linear regression, we assumed a linear model

**2. 2008 US swing state election results**

for how the data are generated and then estimated the parameters that are defined by that model. But, how to we assess how reasonable it is that our observed data are actually described by the model? This is the realm of hypothesis testing. Let's start by thinking about a simpler scenario. Consider the following.

```{python}
a, b = np.polyfit(election.total_votes, election.dem_share, 1)
x = np.array([0, 1000])
y = a * x*900 + b
g = sns.relplot(x = 'total', 
                y = 'dem_share',
                kind = 'scatter',
                data = election,
                height = 2.6)
_ = g.set(xlabel = 'total votes (thousands)',
          ylabel = 'percent of vote for Obama',
          ylim = (0, 100),
          xlim = (0, 1000))
_ = plt.plot(x, y, color = 'green')
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**3. Hypothesis testing**

Ohio and Pennsylvania are similar states. They are neighbors and they both have liberal urban counties and also lots of rural conservative counties. I hypothesize that county-level voting in these two states have identical probability distributions. We have voting data to help test if this hypothesis. Stated more concretely,

**4. Hypothesis testing**

we are going to assess how reasonable the observed data are assuming the hypothesis is true. The hypothesis we are testing is

**5. Null hypothesis**

typically called the null hypothesis. We might start by just plotting the two ECDFs of

6. ECDFs of swing state election results
the county-level votes. Whew! It is pretty tough to make a judgment here. Pennsylvania seems to be slightly more toward Obama in the middle part of the ECDFs, but not much. We can't really draw a conclusion here.

```{python}
def plot_ecdf(x_val, y_val, x_label, y_label, color):
  # Create a Figure and an Axes with plt.subplots
  _ = plt.tight_layout(pad = 1)
  _ = plt.plot(x_val, y_val, marker = '.', linestyle = 'none', color = color)
  _ = plt.xlabel(x_label)
  _ = plt.ylabel(y_label)
  quit()

x_dem_share_PA, y_dem_share_PA = ecdf(dem_share_PA)
x_dem_share_OH, y_dem_share_OH = ecdf(dem_share_OH)

plot_ecdf(x_dem_share_PA, y_dem_share_PA, 'percent of vote for Obama', 'ECDF', color = 'blue')
plot_ecdf(x_dem_share_OH, y_dem_share_OH, 'percent of vote for Obama', 'ECDF', color = 'green')
_ = plt.legend(['PA', 'OH'])
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**7. Percent vote for Obama**

We could just compare some summary statistics. Again, this is a tough call. The means and medians of the two states are really close, and the standard deviations are almost identical. So eyeballing the data is not enough. To resolve this issue,

```{python}
election_summary = pd.pivot_table(election, values = 'dem_share', index = 'state', aggfunc = {'dem_share': [np.mean, np.median, np.std]})
election_summary = election_summary.T
election_summary['PA - OH difference'] = election_summary.PA - election_summary.OH
election_summary.drop(['FL'], axis = 1)
```

1 Data retrieved from Data.gov (https://www.data.gov/)

**8. Simulating the hypothesis**

we can simulate what the data would look like if the county-level voting trends in the two states were identically distributed. We can do this by putting the Democratic share of the vote for all of Pennsylvania's 67 counties and Ohio's 88 counties together.

1 Data retrieved from Data.gov (https://www.data.gov/)

**9. Simulating the hypothesis**

We then ignore what state they belong to. Next, we randomly scramble

1 Data retrieved from Data.gov (https://www.data.gov/)

**10. Simulating the hypothesis**

the ordering of the counties.

1 Data retrieved from Data.gov (https://www.data.gov/)

**11. Simulating the hypothesis**

We then re-label the first 67 to be "Pennsylvania" and the remaining ones to be "Ohio." So, we just redid the election as if there was no difference between Pennsylvania and Ohio.

**12. Permutation**

This technique, of scrambling the order of an array, is called a permutation. It is at the heart of simulating a null hypothesis were we assume two quantities are identically distributed.

**13. Generating a permutation sample**

Let's look at how we can implement this in Python. First, we need to make a single array with all of the counties in it. We do this using the np dot concatenate function. Notice that this function takes a tuple of the arrays you wish to concatenate as an argument. Next, we use the function np dot random dot permutation to conveniently permute the entries of the array. We then assign the first 67 to be labeled Pennsylvania and the last 88 to be labeled Ohio. These samples are called permutation samples.

```{python}
# Seed random number generator
np.random.seed(42)

import numpy as np
dem_share_both = np.concatenate((dem_share_PA, dem_share_OH))
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)]
perm_sample_OH = dem_share_perm[len(dem_share_OH):]
```

**14. Let's practice!**

### **Generating a permutation sample**

In the video, you learned that permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets.

Remember, a permutation sample of two arrays having respectively `n1` and `n2` entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first `n1` entries as the permutation sample of the first array and the last `n2` entries as the permutation sample of the second array.

**Instructions**

- Concatenate the two input arrays into one using `np.concatenate()`. Be sure to pass in `data1` and `data2` as one argument (`data1`, `data2`).
- Use `np.random.permutation()` to permute the concatenated array.
Store the first `len(data1)` entries of `permuted_data` as `perm_sample_1` and the last `len(data2)` entries of `permuted_data` as `perm_sample_2`. In practice, this can be achieved by using `:len(data1)` and `len(data2):` to slice `permuted_data`.
- Return `perm_sample_1` and `perm_sample_2`.

```{python}
def permutation_sample(data1, data2):
  """Generate a permutation sample from two data sets."""
  # Concatenate the data sets: data
  data = np.concatenate((data1, data2))
  # Permute the concatenated array: permuted_data
  permuted_data = np.random.permutation(data)
  # Split the permuted array into two: perm_sample_1, perm_sample_2
  perm_sample_1 = permuted_data[:len(data1)]
  perm_sample_2 = permuted_data[len(data1):]
  return perm_sample_1, perm_sample_2
```

Now, let's practice doing some permutation sampling of real data!

### **Visualizing permutation sampling**

To help see how permutation sampling works, in this exercise you will generate permutation samples and look at them graphically.

We will use the Sheffield Weather Station data again, this time considering the monthly rainfall in June (a dry month) and November (a wet month). We expect these might be differently distributed, so we will take permutation samples to see how their ECDFs *would look if* they were identically distributed.

The data are stored in the Numpy arrays `rain_june` and `rain_november`.

As a reminder, `permutation_sample()` has a function signature of `permutation_sample(data_1, data_2)` with a return value of 
```{python, eval = False}
permuted_data[:len(data_1)],
permuted_data[len(data_1):], 
```
, where 
```{python, eval = False}
permuted_data = np.random.permutation(np.concatenate((data_1, data_2)))
```
.

**Instructions**

- Write a `for` loop to generate 50 permutation samples, compute their ECDFs, and plot them.
  - Generate a permutation sample pair from `rain_june` and `rain_november` using your `permutation_sample()` function.
  - Generate the `x` and `y` values for an ECDF for each of the two permutation samples for the ECDF using your `ecdf()` function.
  - Plot the ECDF of the first permutation sample (`x_1` and `y_1`) as dots. Do the same for the second permutation sample (`x_2` and `y_2`).
- Generate `x` and `y` values for ECDFs for the `rain_june` and `rain_november` data and plot the ECDFs using respectively the keyword arguments `color='red'` and `color='blue'`.
- Label your axes, set a 2% margin, and show your plot. This has been done for you, so just hit 'Submit Answer' to view the plot!

```{python}
for i in range(50):
  # Generate permutation samples
  perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)
  # Compute ECDFs
  x_1, y_1 = ecdf(perm_sample_1)
  x_2, y_2 = ecdf(perm_sample_2)
  # Plot ECDFs of permutation sample
  _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
               color='red', alpha=0.02)
  _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
               color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()
plt.clf()
```

Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed.

## **Test statistics and p-values**

**1. Test statistics and p-values**

Now that we know how to simulate the null hypothesis using permutation, we can start to test it. We will continue our study of hypothesis testing with

**2. Are OH and PA different?**

the Ohio/Pennsylvania vote data. We are testing the null hypothesis that the county-level voting is identically distributed between the two states. Remember that

```{python}
plot_ecdf(x_dem_share_PA, y_dem_share_PA, 'percent of vote for Obama', 'ECDF', color = 'blue')
plot_ecdf(x_dem_share_OH, y_dem_share_OH, 'percent of vote for Obama', 'ECDF', color = 'green')
_ = plt.legend(['PA', 'OH'])
plt.show()
plt.clf()
```

1 Data retrieved from Data.gov (https://www.data.gov/)

**3. Hypothesis testing**

testing a hypothesis is an assessment of how reasonable the observed data are assuming the hypothesis is true. But this is a bit vague. What about the data do we assess and how do we quantify the assessment? The answer to these questions hinges on the concept of

**4. Test statistic**

a test statistic. A test statistic is a single number that can be computed from observed data and also from data you simulate under the null hypothesis. It serves as a basis of comparison between what the hypothesis predicts and what we actually observed. Importantly, you should choose your test statistic to be something that is pertinent to the question you are trying to answer with your hypothesis test, in this case, are the two states different? If they are identical, they should have the same mean vote share for Obama. So the difference in mean vote share should be zero. We will therefore choose the difference in means as our test statistic.

**5. Permutation replicate**

From the permutation sample we generated in the last video, the value of the test statistic is 1-point-12%. The value of a test statistic computed from a permutation sample is called a permutation replicate, in this case, 1-point-12%. We already calculated that the difference in mean vote share from the actual election was 1-point-16%. So, for this permutation replicate, we did not quite get as big of a difference in means than what was observed in the original data. Now, we can "redo" the election 10,000 times under the null hypothesis by generating lots and lots of permutation replicates. (You will write for loops to do this in the exercises.)

```{python}
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)
```
```{python}
np.mean(dem_share_PA) - np.mean(dem_share_OH) # orig. data
```


**6. Mean vote difference under null hypothesis**

We can plot a histogram of all the permutation replicates. The difference of means from the elections simulated under the null hypothesis lies somewhere between -4 and 4%. The actual mean percent vote difference was 1-point-16%, shown by the red line. If we tally up the area of the histogram that is

```{python}
perm_diff_election = np.empty()
for i in range(10000):
  perm_sample_PA, perm_sample_OH = permutation_sample(dem_share_PA, dem_share_OH)
  perm_diff_election[i] =np.mean(perm_sample_PA) - np.mean(perm_sample_OH)

diff_data_election = np.mean(dem_share_PA) - np.mean(dem_share_OH)
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)

# Plot the histogram of the replicates
_ = plt.hist(perm_diff_election, bins = 30, density = True)
_ = plt.axvline(x = diff_data_election, color = 'red')
_ = plt.xlabel('PA - OH mean percent vote difference')
_ = plt.ylabel('PDF')

plt.show()
plt.clf()
```

1 Data retrieved from Data.gov (https://www.data.gov/)

**7. Mean vote difference under null hypothesis**

to the right of the read line, we get that about 23% of the simulated elections had at least a 1-point-16% difference or greater. This value, point-23, is called

```{python}
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)

# Plot the histogram of the replicates
_ = plt.hist(perm_diff_election, bins = 30, density = True)
_ = plt.axvline(x = diff_data_election, color = 'red')
_ = plt.xlabel('PA - OH mean percent vote difference')
_ = plt.ylabel('PDF')

plt.show()
plt.clf()
```

1 Data retrieved from Data.gov (https://www.data.gov/)

**8. p-value**

the p-value. It is the probability of getting at least a 1-point-16% difference in the mean vote share assuming the states have identically distributed voting. So is it plausible that we would observe the vote share we got if Pennsylvania and Ohio had identically distributed county-level voting? Sure it is. It happened 23% of the time under the null hypothesis. Now, we have to be careful about the definition of the p-value. Again, the p-value is the probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true. The p-value is exactly that. It is not the probability that the null hypothesis is true. Further, the p-value is only meaningful if the null hypothesis is clearly stated, along with the test statistic used to evaluate it. When the p-value is small, it is often said that the data are

**9. Statistical significance**

statistically significantly different than what we would observe under the null hypothesis. For this reason, the hypothesis testing we're doing is sometimes called

**10. Null hypothesis significance testing (NHST)**

null hypothesis significance testing, or NHST. I encourage you not to just label something as statistically significant or not, but rather to consider the value of the p-value, as well as how much different the data are from what you would expect from the null hypothesis.

**11. statistical significance ? practical significance**

Remember: statistical significance (that is, low p-values) and practical significance, whether or not the difference of the data from the null hypothesis matters for practical considerations, are two different things.

**12. Let's practice!**

Ok, now let's perform some hypothesis tests!

### **Test statistics**

When performing hypothesis tests, your choice of test statistic should be:

**Possible Answers**

1. something well-known, like the mean or median.

2. be a parameter that can be estimated.

3. be pertinent to the question you are seeking to answer in your hypothesis test.

Answer: 3

The most important thing to consider is: What are you asking?

### **What is a p-value?**

The p-value is generally a measure of:

**Possible Answers**

1. the probability that the hypothesis you are testing is true.

2. the probability of observing your data if the hypothesis you are testing is true.

3. the probability of observing a test statistic equally or more extreme than the one you observed, given that the null hypothesis is true.

Answer: 3

### **Generating permutation replicates**

As discussed in the video, a permutation replicate is a single value of a statistic computed from a permutation sample. As the `draw_bs_reps()` function you wrote in chapter 2 is useful for you to generate bootstrap replicates, it is useful to have a similar function, `draw_perm_reps()`, to generate permutation replicates. You will write this useful function in this exercise.

The function has call signature `draw_perm_reps(data_1, data_2, func, size=1)`. Importantly, func must be a function that takes two arrays as arguments. In most circumstances, `func` will be a function you write yourself.

**Instructions**

- Define a function with this signature: `draw_perm_reps(data_1, data_2, func, size=1)`.
   - Initialize an array to hold the permutation replicates using `np.empty()`.
  - Write a for loop to:
    - Compute a permutation sample using your `permutation_sample()` function
    - Pass the samples into `func()` to compute the replicate and store the result in your array of replicates.
  - Return the array of replicates.

```{python}
def draw_perm_reps(data_1, data_2, func, size=1):
  """Generate multiple permutation replicates."""
  # Initialize array of replicates: perm_replicates
  perm_replicates = np.empty(size)
  for i in range(size):
      # Generate permutation sample
      perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)
      # Compute the test statistic
      perm_replicates[i] = func(perm_sample_1, perm_sample_2)
  return perm_replicates
```

### **Look before you leap: EDA before hypothesis testing**

Kleinteich and Gorb (Sci. Rep., **4**, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog's tongue when it struck the target.

Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let's make a bee swarm plot for the data. They are stored in a Pandas data frame, `df`, where column `ID` is the identity of the frog and column `impact_force` is the impact force in Newtons (N).

**Instructions**

- Use `sns.swarmplot()` to make a bee swarm plot of the data by specifying the `x`, `y`, and data keyword arguments.
- Label your axes.
- Show the plot.

```{python}
# Make bee swarm plot
_ = sns.swarmplot(x = "ID", y = 'impact_force', data = df)

# Label axes
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')

# Show the plot
plt.show()
plt.clf()
```

Eyeballing it, it does not look like they come from the same distribution. Frog A, the adult, has three or four very hard strikes, and Frog B, the juvenile, has a couple weak ones. However, it is possible that with only 20 samples it might be too difficult to tell if they have difference distributions, so we should proceed with the hypothesis test.

### **Permutation test on frog data**

The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis.

For your convenience, the data has been stored in the arrays `force_a` and `force_b`.

**Instructions**

- Define a function with call signature `diff_of_means(data_1, data_2)` that returns the differences in means between two data sets, mean of `data_1` minus mean of `data_2`.
- Use this function to compute the empirical difference of means that was observed in the frogs.
- Draw 10,000 permutation replicates of the difference of means.
- Compute the p-value.
- Print the p-value.

```{python}
def diff_of_means(data_1, data_2):
  """Difference in means of two arrays."""
  # The difference of means of data_1, data_2: diff
  diff = np.mean(data_1) - np.mean(data_2)
  return diff

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = diff_of_means(force_a, force_b)

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(force_a, force_b,
                                 diff_of_means, size = 10000)

# Compute p-value: p
p = np.sum(perm_replicates >= empirical_diff_means) / len(perm_replicates)

# Print the result
print('p-value =', p)
```

The p-value tells you that there is about a 0.6% chance that you would get the difference of means observed in the experiment if frogs were exactly the same. A p-value below 0.01 is typically said to be "statistically significant," but: warning! warning! warning! You have computed a p-value; it is a number. I encourage you not to distill it to a yes-or-no phrase. p = 0.006 and p = 0.000000006 are both said to be "statistically significant," but they are definitely not the same!

## **Bootstrap hypothesis tests**

**1. Bootstrap hypothesis tests**

Let's go over the pipeline of hypothesis testing that we have been studying.

**2. Pipeline for hypothesis testing**

First, clearly state the null hypothesis. Stating the null hypothesis so that it is crystal clear is essential to be able to simulate it. Next, define your test statistic. Then generate many sets of simulated data assuming the null hypothesis is true. Compute the test statistic for each simulated data set. The p-value is then the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data. Now let's do another hypothesis test.

**3. Michelson and Newcomb: speed of light pioneers**

Consider again Michelson's measurements of the speed of light. Around the same time that Michelson did his experiment, his future collaborator Simon Newcomb also measured the speed of light.

1 Michelson image: public domain, Smithsonian
2 Newcomb image: US Library of Congress

**4. Michelson and Newcomb: speed of light pioneers**

Newcomb's measurements had a mean of 299,860 km/s, differing from Michelson's by about 8 km/s. We want to know if there is something fundamentally different about Newcomb's and Michelson's measurements. The thing is: we only have Newcomb's mean and none of his data points!

1 Michelson image: public domain, Smithsonian
2 Newcomb image: US Library of Congress

**5. The data we have**

The question is: could Michelson have gotten the data set he did if the true mean speed of light in his experiments was equal to Newcomb's? So, we formally

```{python}
x_vel_light, y_vel_light = ecdf(michelson_speed_of_light)

# Create a Figure and an Axes with plt.subplots
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)

_ = ax.plot(x_vel_light, y_vel_light, marker = '.', linestyle = 'none')

_ = ax.set_xlabel('speed of light (km/s)')
_ = ax.set_ylabel('CDF')
plt.show()
plt.clf()
print("Newcomb's mean = 299,860")
```

1 Data: Michelson, 1880

**6. Null hypothesis**

state our hypothesis as this: the true mean speed of light in Michelson's experiments was actually Newcomb's reported mean, which we'll call the Newcomb value. When I say the true mean speed of light in Michelson's experiments, think the mean Michelson would have gotten had done his experiment lots and lots and lots of times. Because we are comparing a data set with a value, a permutation test is not applicable. We need to simulate the situation in which the true mean speed of light in Michelson's experiments is Newcomb's value.

**7. Shifting the Michelson data**

To achieve this, we shift Michelson's data such that its mean now matches Newcomb's value. See here the ECDF of the shifted data relative to the original data. We can then use bootstrapping on this shifted data to simulate data acquisition under the null hypothesis.

```{python}
newcomb_value = 299860
michelson_shifted = michelson_speed_of_light - 8*(- np.mean(michelson_speed_of_light) + newcomb_value)
          
#michelson_shifted_data = np.random.normal(newcomb_value,  np.std(michelson_speed_of_light), len(michelson_speed_of_light))
x_vel_light_shifted, y_vel_light_shifted = ecdf(michelson_shifted)

# Create a Figure and an Axes with plt.subplots
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)

_ = ax.plot(x_vel_light, y_vel_light, marker = '.', linestyle = 'none')
_ = ax.plot(x_vel_light_shifted, y_vel_light_shifted, marker = '.', linestyle = 'none', color = 'green')

_ = ax.set_xlabel('speed of light (km/s)')
_ = ax.set_ylabel('CDF')

# Annotate data
_ = ax.annotate("Michelson's data \nwith same mean \nas Newcomb", xy = (np.median(michelson_shifted), 0.45), xytext = (299600, 0.8), arrowprops = {'arrowstyle': '->', 'color': 'blue'})
_ = ax.annotate("Michelson's data", xy = (np.median(michelson_speed_of_light), 0.55), xytext = (299900, 0.25), arrowprops = {'arrowstyle': '->', 'color': 'blue'})

plt.show()
plt.clf()

```

**8. Calculating the test statistic**

The test statistic is the the mean of the bootstrap sample minus Newcomb's value. We write a function diff_from_newcomb to compute it, and compute the observed test statistic.

```{python}
def diff_from_newcomb(data, newcomb_value = 299860):
  return np.mean(data) - newcomb_value

diff_obs = diff_from_newcomb(michelson_speed_of_light)
diff_obs
```

**9. Computing the p-value**

We then use the draw_bs_reps function you have already written to generate the bootstrap replicates, which is the value of the test statistic computed from a bootstrap sample. Note that the data we pass into the function are the shifted Michelson measurements because those are the ones we use to simulate the null hypothesis. The p-value is computed exactly the same way as for the permutation test. We report the fraction of bootstrap replicates that are less than the observed test statistic. In this case, we use less than because the mean from Michelson's experiments was less than Newcomb's value. We get a p-value of 0-point-16. This suggests that it is quite possible the Newcomb and Michelson did not really have fundamental differences in their measurements. This is an example of

```{python}
michelson_shifted = michelson_speed_of_light + (-np.mean(michelson_speed_of_light) + newcomb_value)
bs_replicates = draw_bs_reps(michelson_shifted, diff_from_newcomb, 10000)
p_value= np.sum(bs_replicates <= diff_obs) / len(bs_replicates)
p_value
```

**10. One sample test**

a one-sample test, since we had one set of samples from Michelson and were comparing to a single number from Newcomb. Often in the field, you will do two-sample tests that require the bootstrap, which you will explore in the exercises. I know this video was a lot to take in. Explicitly simulating a null hypothesis like this, we we have to shift the mean, can be tricky. You'll get a chance to practice in the exercises, and you may want to go over the procedure again to make sure you understand it.

**11. Let's practice!**

Ok, enough talk. Let's do some bootstrap hypothesis tests!

### **A one-sample bootstrap hypothesis test**

Another juvenile frog was studied, Frog C, and you want to see if Frog B and Frog C have similar impact forces. Unfortunately, you do not have Frog C's impact forces available, but you know they have a mean of 0.55 N. Because you don't have the original data, you cannot do a permutation test, and you cannot assess the hypothesis that the forces from Frog B and Frog C come from the same distribution. You will therefore test another, less restrictive hypothesis: The mean strike force of Frog B is equal to that of Frog C.

To set up the bootstrap hypothesis test, you will take the mean as our test statistic. Remember, your goal is to calculate the probability of getting a mean impact force less than or equal to what was observed for Frog B *if the hypothesis that the true mean of Frog B's impact forces is equal to that of Frog C is true*. You first translate all of the data of Frog B such that the mean is 0.55 N. This involves adding the mean force of Frog C and subtracting the mean force of Frog B from each measurement of Frog B. This leaves other properties of Frog B's distribution, such as the variance, unchanged.

**Instructions**

- Translate the impact forces of Frog B such that its mean is 0.55 N.
- Use your `draw_bs_reps()` function to take 10,000 bootstrap replicates of the mean of your translated forces.
- Compute the p-value by finding the fraction of your bootstrap replicates that are less than the observed mean impact force of Frog B. Note that the variable of interest here is `force_b`.
- Print your p-value.

```{python}
# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b + (- np.mean(force_b) + 0.55)

# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)

# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000

# Print the p-value
print('p = ', p)

```

The low p-value suggests that the null hypothesis that Frog B and Frog C have the same mean impact force is false.

### **A two-sample bootstrap hypothesis test for difference of means**

We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, which is also impossible with a permutation test.

To do the two-sample bootstrap test, we shift *both* arrays to have the same mean, since we are simulating the hypothesis that their means are, in fact, equal. We then draw bootstrap samples out of the shifted arrays and compute the difference in means. This constitutes a bootstrap replicate, and we generate many of them. The p-value is the fraction of replicates with a difference in means greater than or equal to what was observed.

The objects `forces_concat` and `empirical_diff_means` are already in your namespace.

**Instructions**

- Compute the mean of all forces (from `forces_concat`) using `np.mean()`.
- Generate shifted data sets for both `force_a` and `force_b` such that the mean of each is the mean of the concatenated array of impact forces.
- Generate 10,000 bootstrap replicates of the mean each for the two shifted arrays.
- Compute the bootstrap replicates of the difference of means by subtracting the replicates of the shifted impact force of Frog B from those of Frog A.
- Compute and print the p-value from your bootstrap replicates.

```{python}
# Compute mean of all forces: mean_force
mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force 

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates >= empirical_diff_means) / len(bs_replicates)
print('p-value =', p)
```

You got a similar result as when you did the permutation test. Nonetheless, remember that it is important to carefully think about what question you want to ask. Are you only interested in the mean impact force, or in the distribution of impact forces?