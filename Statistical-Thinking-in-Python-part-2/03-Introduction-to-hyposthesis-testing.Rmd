---
title: "03 - Introduction to hyposthesis testing"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
    code_folding: hide
    df_print: kable
---

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Set default Seaborn style
sns.set()

election = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/2008_election_results_swing.csv")
election['total'] = election.total_votes/900
total = election.total_votes.values/900
dem_share = election.dem_share.values
dem_share_PA = election.dem_share.values[(election.state == 'PA')]
dem_share_OH = election.dem_share.values[(election.state == 'OH')]

sheffield_weather_station = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/sheffield_weather_station.csv")
rain_june = sheffield_weather_station.rain.values[(sheffield_weather_station.mm == 6)]
rain_november = sheffield_weather_station.rain.values[(sheffield_weather_station.mm == 11)]

def ecdf(data):
  """Compute ECDF for a one-dimensional array of measurements."""
  # Number of data points: n
  n = len(data)
  # x-data for the ECDF: x
  x = np.sort(data)
  # y-data for the ECDF: y
  y = np.arange(1, n + 1) / n
  return x, y

```

## **Formulating and simulating a hypothesis**

**1. Formulating and simulating a hypothesis**

When we studied linear regression, we assumed a linear model

**2. 2008 US swing state election results**

for how the data are generated and then estimated the parameters that are defined by that model. But, how to we assess how reasonable it is that our observed data are actually described by the model? This is the realm of hypothesis testing. Let's start by thinking about a simpler scenario. Consider the following.

```{python}
a, b = np.polyfit(election.total_votes, election.dem_share, 1)
x = np.array([0, 1000])
y = a * x*900 + b
g = sns.relplot(x = 'total', 
                y = 'dem_share',
                kind = 'scatter',
                data = election,
                height = 2.6)
_ = g.set(xlabel = 'total votes (thousands)',
          ylabel = 'percent of vote for Obama',
          ylim = (0, 100),
          xlim = (0, 1000))
_ = plt.plot(x, y)
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**3. Hypothesis testing**

Ohio and Pennsylvania are similar states. They are neighbors and they both have liberal urban counties and also lots of rural conservative counties. I hypothesize that county-level voting in these two states have identical probability distributions. We have voting data to help test if this hypothesis. Stated more concretely,

**4. Hypothesis testing**

we are going to assess how reasonable the observed data are assuming the hypothesis is true. The hypothesis we are testing is

**5. Null hypothesis**

typically called the null hypothesis. We might start by just plotting the two ECDFs of

6. ECDFs of swing state election results
the county-level votes. Whew! It is pretty tough to make a judgment here. Pennsylvania seems to be slightly more toward Obama in the middle part of the ECDFs, but not much. We can't really draw a conclusion here.

```{python}
def plot_ecdf(x_val, y_val, x_label, y_label):
  # Create a Figure and an Axes with plt.subplots
  _ = plt.tight_layout(pad = 1)
  _ = plt.plot(x_val, y_val, marker = '.', linestyle = 'none')
  _ = plt.xlabel(x_label)
  _ = plt.ylabel(y_label)
  quit()

x_dem_share_PA, y_dem_share_PA = ecdf(dem_share_PA)
x_dem_share_OH, y_dem_share_OH = ecdf(dem_share_OH)

plot_ecdf(x_dem_share_PA, y_dem_share_PA, 'percent of vote for Obama', 'ECDF')
plot_ecdf(x_dem_share_OH, y_dem_share_OH, 'percent of vote for Obama', 'ECDF')
_ = plt.legend(['PA', 'OH'])
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**7. Percent vote for Obama**

We could just compare some summary statistics. Again, this is a tough call. The means and medians of the two states are really close, and the standard deviations are almost identical. So eyeballing the data is not enough. To resolve this issue,

```{python}
election_summary = pd.pivot_table(election, values = 'dem_share', index = 'state', aggfunc = {'dem_share': [np.mean, np.median, np.std]})
election_summary = election_summary.T
election_summary['PA - OH difference'] = election_summary.PA - election_summary.OH
election_summary.drop(['FL'], axis = 1)
```

1 Data retrieved from Data.gov (https://www.data.gov/)

**8. Simulating the hypothesis**

we can simulate what the data would look like if the county-level voting trends in the two states were identically distributed. We can do this by putting the Democratic share of the vote for all of Pennsylvania's 67 counties and Ohio's 88 counties together.

1 Data retrieved from Data.gov (https://www.data.gov/)

**9. Simulating the hypothesis**

We then ignore what state they belong to. Next, we randomly scramble

1 Data retrieved from Data.gov (https://www.data.gov/)

**10. Simulating the hypothesis**

the ordering of the counties.

1 Data retrieved from Data.gov (https://www.data.gov/)

**11. Simulating the hypothesis**

We then re-label the first 67 to be "Pennsylvania" and the remaining ones to be "Ohio." So, we just redid the election as if there was no difference between Pennsylvania and Ohio.

**12. Permutation**

This technique, of scrambling the order of an array, is called a permutation. It is at the heart of simulating a null hypothesis were we assume two quantities are identically distributed.

**13. Generating a permutation sample**

Let's look at how we can implement this in Python. First, we need to make a single array with all of the counties in it. We do this using the np dot concatenate function. Notice that this function takes a tuple of the arrays you wish to concatenate as an argument. Next, we use the function np dot random dot permutation to conveniently permute the entries of the array. We then assign the first 67 to be labeled Pennsylvania and the last 88 to be labeled Ohio. These samples are called permutation samples.

```{python}
import numpy as np
dem_share_both = np.concatenate((dem_share_PA, dem_share_OH))
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)]
perm_sample_OH = dem_share_perm[len(dem_share_OH):]
```

**14. Let's practice!**

### **Generating a permutation sample**

In the video, you learned that permutation sampling is a great way to simulate the hypothesis that two variables have identical probability distributions. This is often a hypothesis you want to test, so in this exercise, you will write a function to generate a permutation sample from two data sets.

Remember, a permutation sample of two arrays having respectively `n1` and `n2` entries is constructed by concatenating the arrays together, scrambling the contents of the concatenated array, and then taking the first `n1` entries as the permutation sample of the first array and the last `n2` entries as the permutation sample of the second array.

**Instructions**

- Concatenate the two input arrays into one using `np.concatenate()`. Be sure to pass in `data1` and `data2` as one argument (`data1`, `data2`).
- Use `np.random.permutation()` to permute the concatenated array.
Store the first `len(data1)` entries of `permuted_data` as `perm_sample_1` and the last `len(data2)` entries of `permuted_data` as `perm_sample_2`. In practice, this can be achieved by using `:len(data1)` and `len(data2):` to slice `permuted_data`.
- Return `perm_sample_1` and `perm_sample_2`.

```{python}
def permutation_sample(data1, data2):
  """Generate a permutation sample from two data sets."""
  # Concatenate the data sets: data
  data = np.concatenate((data1, data2))
  # Permute the concatenated array: permuted_data
  permuted_data = np.random.permutation(data)
  # Split the permuted array into two: perm_sample_1, perm_sample_2
  perm_sample_1 = permuted_data[:len(data1)]
  perm_sample_2 = permuted_data[len(data1):]
  return perm_sample_1, perm_sample_2
```

Now, let's practice doing some permutation sampling of real data!

### **Visualizing permutation sampling**

To help see how permutation sampling works, in this exercise you will generate permutation samples and look at them graphically.

We will use the Sheffield Weather Station data again, this time considering the monthly rainfall in June (a dry month) and November (a wet month). We expect these might be differently distributed, so we will take permutation samples to see how their ECDFs *would look if* they were identically distributed.

The data are stored in the Numpy arrays `rain_june` and `rain_november`.

As a reminder, `permutation_sample()` has a function signature of `permutation_sample(data_1, data_2)` with a return value of 
```{python, eval = False}
permuted_data[:len(data_1)],
permuted_data[len(data_1):], 
```
, where 
```{python}
permuted_data = np.random.permutation(np.concatenate((data_1, data_2)))
```
.

**Instructions**

**Instructions**

- Write a `for` loop to generate 50 permutation samples, compute their ECDFs, and plot them.
  - Generate a permutation sample pair from `rain_june` and `rain_november` using your `permutation_sample()` function.
  - Generate the `x` and `y` values for an ECDF for each of the two permutation samples for the ECDF using your `ecdf()` function.
  - Plot the ECDF of the first permutation sample (`x_1` and `y_1`) as dots. Do the same for the second permutation sample (`x_2` and `y_2`).
- Generate `x` and `y` values for ECDFs for the `rain_june` and `rain_november` data and plot the ECDFs using respectively the keyword arguments `color='red'` and `color='blue'`.
- Label your axes, set a 2% margin, and show your plot. This has been done for you, so just hit 'Submit Answer' to view the plot!

```{python}
for i in range(50):
  # Generate permutation samples
  perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)
  # Compute ECDFs
  x_1, y_1 = ecdf(perm_sample_1)
  x_2, y_2 = ecdf(perm_sample_2)
  # Plot ECDFs of permutation sample
  _ = plt.plot(x_1, y_1, marker='.', linestyle='none',
               color='red', alpha=0.02)
  _ = plt.plot(x_2, y_2, marker='.', linestyle='none',
               color='blue', alpha=0.02)

# Create and plot ECDFs from original data
x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()
```

Notice that the permutation samples ECDFs overlap and give a purple haze. None of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis is not commensurate with the data. June and November rainfall are not identically distributed.

## **Test statistics and p-values**

**1. Test statistics and p-values**

Now that we know how to simulate the null hypothesis using permutation, we can start to test it. We will continue our study of hypothesis testing with

**2. Are OH and PA different?**

the Ohio/Pennsylvania vote data. We are testing the null hypothesis that the county-level voting is identically distributed between the two states. Remember that

1 Data retrieved from Data.gov (https://www.data.gov/)

**3. Hypothesis testing**

testing a hypothesis is an assessment of how reasonable the observed data are assuming the hypothesis is true. But this is a bit vague. What about the data do we assess and how do we quantify the assessment? The answer to these questions hinges on the concept of

**4. Test statistic**

a test statistic. A test statistic is a single number that can be computed from observed data and also from data you simulate under the null hypothesis. It serves as a basis of comparison between what the hypothesis predicts and what we actually observed. Importantly, you should choose your test statistic to be something that is pertinent to the question you are trying to answer with your hypothesis test, in this case, are the two states different? If they are identical, they should have the same mean vote share for Obama. So the difference in mean vote share should be zero. We will therefore choose the difference in means as our test statistic.

**5. Permutation replicate**

From the permutation sample we generated in the last video, the value of the test statistic is 1-point-12%. The value of a test statistic computed from a permutation sample is called a permutation replicate, in this case, 1-point-12%. We already calculated that the difference in mean vote share from the actual election was 1-point-16%. So, for this permutation replicate, we did not quite get as big of a difference in means than what was observed in the original data. Now, we can "redo" the election 10,000 times under the null hypothesis by generating lots and lots of permutation replicates. (You will write for loops to do this in the exercises.)

**6. Mean vote difference under null hypothesis**

We can plot a histogram of all the permutation replicates. The difference of means from the elections simulated under the null hypothesis lies somewhere between -4 and 4%. The actual mean percent vote difference was 1-point-16%, shown by the red line. If we tally up the area of the histogram that is

1 Data retrieved from Data.gov (https://www.data.gov/)

**7. Mean vote difference under null hypothesis**

to the right of the read line, we get that about 23% of the simulated elections had at least a 1-point-16% difference or greater. This value, point-23, is called

1 Data retrieved from Data.gov (https://www.data.gov/)

**8. p-value**

the p-value. It is the probability of getting at least a 1-point-16% difference in the mean vote share assuming the states have identically distributed voting. So is it plausible that we would observe the vote share we got if Pennsylvania and Ohio had identically distributed county-level voting? Sure it is. It happened 23% of the time under the null hypothesis. Now, we have to be careful about the definition of the p-value. Again, the p-value is the probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true. The p-value is exactly that. It is not the probability that the null hypothesis is true. Further, the p-value is only meaningful if the null hypothesis is clearly stated, along with the test statistic used to evaluate it. When the p-value is small, it is often said that the data are

**9. Statistical significance**

statistically significantly different than what we would observe under the null hypothesis. For this reason, the hypothesis testing we're doing is sometimes called

**10. Null hypothesis significance testing (NHST)**

null hypothesis significance testing, or NHST. I encourage you not to just label something as statistically significant or not, but rather to consider the value of the p-value, as well as how much different the data are from what you would expect from the null hypothesis.

**11. statistical significance ? practical significance**

Remember: statistical significance (that is, low p-values) and practical significance, whether or not the difference of the data from the null hypothesis matters for practical considerations, are two different things.

**12. Let's practice!**

Ok, now let's perform some hypothesis tests!

### **Test statistics**

When performing hypothesis tests, your choice of test statistic should be:

**Possible Answers**

1. something well-known, like the mean or median.

2. be a parameter that can be estimated.

3. be pertinent to the question you are seeking to answer in your hypothesis test.

Answer:

### **Generating permutation replicates**

As discussed in the video, a permutation replicate is a single value of a statistic computed from a permutation sample. As the `draw_bs_reps()` function you wrote in chapter 2 is useful for you to generate bootstrap replicates, it is useful to have a similar function, `draw_perm_reps()`, to generate permutation replicates. You will write this useful function in this exercise.

The function has call signature `draw_perm_reps(data_1, data_2, func, size=1)`. Importantly, func must be a function that takes two arrays as arguments. In most circumstances, `func` will be a function you write yourself.

**Instructions**

- Define a function with this signature: `draw_perm_reps(data_1, data_2, func, size=1)`.
   - Initialize an array to hold the permutation replicates using `np.empty()`.
  - Write a for loop to:
    - Compute a permutation sample using your `permutation_sample()` function
    - Pass the samples into `func()` to compute the replicate and store the result in your array of replicates.
  - Return the array of replicates.

```{python}
def draw_perm_reps(data_1, data_2, func, size=1):
    """Generate multiple permutation replicates."""

    # Initialize array of replicates: perm_replicates
    perm_replicates = ____

    for i in ____:
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = ____

        # Compute the test statistic
        perm_replicates[i] = ____

    return perm_replicates
```

### **Look before you leap: EDA before hypothesis testing
Kleinteich and Gorb (Sci. Rep., 4, 5225, 2014) performed an interesting experiment with South American horned frogs. They held a plate connected to a force transducer, along with a bait fly, in front of them. They then measured the impact force and adhesive force of the frog's tongue when it struck the target.

Frog A is an adult and Frog B is a juvenile. The researchers measured the impact force of 20 strikes for each frog. In the next exercise, we will test the hypothesis that the two frogs have the same distribution of impact forces. But, remember, it is important to do EDA first! Let's make a bee swarm plot for the data. They are stored in a Pandas data frame, `df`, where column `ID` is the identity of the frog and column `impact_force` is the impact force in Newtons (N).

**Instructions**

- Use `sns.swarmplot()` to make a bee swarm plot of the data by specifying the `x`, `y`, and data keyword arguments.
- Label your axes.
- Show the plot.

```{python}
# Make bee swarm plot
_ = ____

# Label axes
_ = plt.____('frog')
_ = plt.____('impact force (N)')

# Show the plot
____

```

### **Permutation test on frog data**

The average strike force of Frog A was 0.71 Newtons (N), and that of Frog B was 0.42 N for a difference of 0.29 N. It is possible the frogs strike with the same force and this observed difference was by chance. You will compute the probability of getting at least a 0.29 N difference in mean strike force under the hypothesis that the distributions of strike forces for the two frogs are identical. We use a permutation test with a test statistic of the difference of means to test this hypothesis.

For your convenience, the data has been stored in the arrays `force_a` and `force_b`.

**Instructions**

- Define a function with call signature `diff_of_means(data_1, data_2)` that returns the differences in means between two data sets, mean of `data_1` minus mean of `data_2`.
- Use this function to compute the empirical difference of means that was observed in the frogs.
- Draw 10,000 permutation replicates of the difference of means.
- Compute the p-value.
- Print the p-value.

```{python}
def diff_of_means(data_1, data_2):
    """Difference in means of two arrays."""

    # The difference of means of data_1, data_2: diff
    diff = ____

    return diff

# Compute difference of mean impact force from experiment: empirical_diff_means
empirical_diff_means = ____

# Draw 10,000 permutation replicates: perm_replicates
perm_replicates = draw_perm_reps(____, ____,
                                 ____, size=10000)

# Compute p-value: p
p = np.sum(____ >= ____) / len(____)

# Print the result
print('p-value =', p)
```

## **Bootstrap hypothesis tests**

