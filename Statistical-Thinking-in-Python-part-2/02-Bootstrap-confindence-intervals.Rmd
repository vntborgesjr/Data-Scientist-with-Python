---
title: "02 - Bootstrap confindence intervals"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
    code_folding: hide
---

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Set default Seaborn style
sns.set()

light = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/speed-of-light.csv")
michelson_speed_of_light = np.array(light['velocity of light in air (km/s)'])

sheffield_weather_station = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/sheffield_weather_station.csv")

rainfall = sheffield_weather_station.rain.values

def ecdf(data):
  """Compute ECDF for a one-dimensional array of measurements."""
  # Number of data points: n
  n = len(data)
  # x-data for the ECDF: x
  x = np.sort(data)
  # y-data for the ECDF: y
  y = np.arange(1, n + 1) / n
  return x, y

nohitter_times = np.array([ 843, 1613, 1101,  215,  684,  814,  278,  324,  161,  219,  545,
        715,  966,  624,   29,  450,  107,   20,   91, 1325,  124, 1468,
        104, 1309,  429,   62, 1878, 1104,  123,  251,   93,  188,  983,
        166,   96,  702,   23,  524,   26,  299,   59,   39,   12,    2,
        308, 1114,  813,  887,  645, 2088,   42, 2090,   11,  886, 1665,
       1084, 2900, 2432,  750, 4021, 1070, 1765, 1322,   26,  548, 1525,
         77, 2181, 2752,  127, 2147,  211,   41, 1575,  151,  479,  697,
        557, 2267,  542,  392,   73,  603,  233,  255,  528,  397, 1529,
       1023, 1194,  462,  583,   37,  943,  996,  480, 1497,  717,  224,
        219, 1531,  498,   44,  288,  267,  600,   52,  269, 1086,  386,
        176, 2199,  216,   54,  675, 1243,  463,  650,  171,  327,  110,
        774,  509,    8,  197,  136,   12, 1124,   64,  380,  811,  232,
        192,  731,  715,  226,  605,  539, 1491,  323,  240,  179,  702,
        156,   82, 1397,  354,  778,  603, 1001,  385,  986,  203,  149,
        576,  445,  180, 1403,  252,  675, 1351, 2983, 1568,   45,  899,
       3260, 1025,   31,  100, 2055, 4043,   79,  238, 3931, 2351,  595,
        110,  215,    0,  563,  206,  660,  242,  577,  179,  157,  192,
        192, 1848,  792, 1693,   55,  388,  225, 1134, 1172, 1555,   31,
       1582, 1044,  378, 1687, 2915,  280,  765, 2819,  511, 1521,  745,
       2491,  580, 2072, 6450,  578,  745, 1075, 1103, 1549, 1520,  138,
       1202,  296,  277,  351,  391,  950,  459,   62, 1056, 1128,  139,
        420,   87,   71,  814,  603, 1349,  162, 1027,  783,  326,  101,
        876,  381,  905,  156,  419,  239,  119,  129,  467])

election = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/2008_election_results_swing.csv")
total = election.total_votes.values/900
dem_share = election.dem_share.values

female_literacy_fertility = pd.read_csv("/home/cla/Documentos/Vitor/DataCamp/Data-Scientist-with-Python/Datasets/female_literacy_fertility.csv")

female_literacy_fertility = female_literacy_fertility[(female_literacy_fertility['fertility'] > 1000)]
illiteracy = 100 - np.array(female_literacy_fertility['female literacy'])
fertility = np.array(female_literacy_fertility['fertility'])/1000

def ecdf(data):
  """Compute ECDF for a one-dimensional array of measurements."""
  # Number of data points: n
  n = len(data)
  # x-data for the ECDF: x
  x = np.sort(data)
  # y-data for the ECDF: y
  y = np.arange(1, n + 1) / n
  return x, y


```

## **Generating bootstrap replicates**

**1. Generating bootstrap replicates**

In the prequel to this course we computed summary statistics of measurements, including the mean, median, and standard deviation. But remember, we need to think probabilistically. What if we acquired the data again? Would we get the same mean? The same median? The same standard deviation? Probably not. In inference problems, it is rare that we are interested in the result from a single experiment or data acquisition. We want to say something more general.

**2. Michelson's speed of light measurements**

Michelson was not interested in what the measured speed of light was in the specific 100 measurements conducted in the summer of 1879. He wanted to know what the speed of light actually is. Statistically speaking, that means he wanted to know what speed of light he would observe if he did the experiment over and over again an infinite number of times. Unfortunately, actually repeating the experiment lots and lots of times is just not possible. But, as hackers, we can simulate getting the data again.

```{python}
mean = np.mean(michelson_speed_of_light)
std = np.std(michelson_speed_of_light)
samples = np.random.normal(mean, std, 10000)
# Create a Figure and an Axes with plt.subplots
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)
sns.distplot(michelson_speed_of_light, hist = True, kde = True, 
             bins = 9, color = 'darkblue', 
             hist_kws = {'edgecolor':'black'},
             kde_kws = {'linewidth': 4})
#_ = ax.hist(michelson_speed_of_light, bins =  9, density = True)
#_ = ax.hist(samples, bins =  int(180/5), density = True, histtype = 'step')
_ = ax.set_xlabel('speed of light(km/s)')
_ = ax.set_ylabel('PDF')
plt.show()
plt.clf()
```

1 Data: Michelson, 1880

**3. Resampling an array**

The idea is that we resample the data we have and recompute the summary statistic of interest, say the mean. To resample an array of measurements, we randomly

Data:
`[23.3, 27.1, 24.3, 25.3, 26.0]`
Mean = `np.mean([23.3, 27.1, 24.3, 25.3, 26.0])`
Resample data: 
`[     ,       ,       ,     ,     ]`
**4. Resampling an array**

select one entry and

Data:
`[23.3, 27.1, 24.3, 25.3, 26.0]`
Mean = `np.mean([23.3, 27.1, 24.3, 25.3, 26.0])`
Resample data: 
`[27.1,       ,       ,     ,     ]`

**5. Resampling an array**

store it. Importantly, we


**6. Resampling an array**

replace the entry in the original array, or equivalently, we just don't delete it. This is called sampling with replacement. Then, we then randomly

**7. Resampling an array**

select another

Data:
`[23.3, 27.1, 24.3, 25.3, 26.0]`
Mean = `np.mean([23.3, 27.1, 24.3, 25.3, 26.0])`
Resample data: 
`[27.1, 26.0,       ,     ,     ]`

**8. Resampling an array**

one and store it. We do this n times,

**9. Resampling an array**

where n is the total number of measurements, five in this case. We then have a resampled array of data. Using this new resampled array, we compute the summary statistic and store the result. Resampling the speed of light data is as if we repeated Michelson's set of measurements.

Data:
`[23.3, 27.1, 24.3, 25.3, 26.0]`
Mean = `np.mean([23.3, 27.1, 24.3, 25.3, 26.0])`
Resample data: 
`[27.1, 26.0, 23.3, 25.7, 23.3]`
Mean = `np.mean([27.1, 26.0, 23.3, 25.7, 23.3])`

**10. Mean of resampled Michelson measurements**

We do this over and over again to get a large number of summary statistics from resampled data sets. We can use these results to plot an ECDF, for example, to get a picture of the probability distribution describing the summary statistic. This process is an example of

```{python}
# Create a Figure and an Axes with plt.subplots
fig, ax = plt.subplots()
fig.tight_layout(pad = 5)

for bs in range(50):
  boots_speed_of_light = np.random.choice(michelson_speed_of_light, size = 100)
  boots_speed_of_light_x, boots_speed_of_light_y = ecdf(boots_speed_of_light)
  _ = ax.plot(boots_speed_of_light_x, boots_speed_of_light_y, marker = '.', linestyle = 'none')

_ = ax.set_xlabel('mean speed of light (km/s)')
_ = ax.set_ylabel('ECDF of bootstrap replicates')
plt.show()
plt.clf()
```

**11. Bootstrapping**

bootstrapping, which more generally is the use of resampled data to perform statistical inference. To make sure we have our terminology down, each resampled array is called

**12. Bootstrap sample**

a bootstrap sample. A bootstrap replicate

**13. Bootstrap replicate**

is the value of the summary statistic computed from the bootstrap sample. The name makes sense; it's a simulated replica of the original data acquired by bootstrapping. Let's look at how we can generate a bootstrap sample and compute a bootstrap replicate from it using Python. We will use Michelson's measurements of the speed of light.

**14. Resampling engine: np.random.choice()**

First, we need a function to perform the resampling. The Numpy function random dot choice provides this functionality. Conveniently, like many of the other functions in the Numpy random module, it has a size keyword argument, which allows us to specify how many samples we want to take out of the array. Notice that it chose the number five three times; the function does not delete an entry when it samples it out of the array. Now, we can draw 100 samples out of the Michelson speed of light data.

```{python}
import numpy as np
np.random.choice([1, 2, 3, 4, 5], size = 5)
```

**15. Computing a bootstrap replicate**

This is a bootstrap sample, since there were 100 data points and we are choosing 100 of them with replacement. Now that we have a bootstrap sample, we can compute a bootstrap replicate. We can pick whatever summary statistic we like. We'll compute the mean, median, and standard deviation. It's as simple as treating the bootstrap sample as though it were a data set.

```{python}
bs_sample = np.random.choice(michelson_speed_of_light, size = 100)
np.mean(bs_sample)
np.median(bs_sample)
np.std(bs_sample)
```

**16. Let's practice!**

Now it's time for you to do some bootstrap sampling yourself!

### **Getting the terminology down**

Getting tripped up over terminology is a common cause of frustration in students. Unfortunately, you often will read and hear other data scientists using different terminology for bootstrap samples and replicates. This is even more reason why we need everything to be clear and consistent for this course. So, before going forward discussing bootstrapping, let's get our terminology down. If we have a data set with $n$ repeated measurements, a **bootstrap sample** is an array of length $n$ that was drawn from the original data with replacement. What is a **bootstrap replicate**?

**Possible Answers**

1. Just another name for a bootstrap sample.

2. A single value of a statistic computed from a bootstrap sample.

3. An actual repeat of the measurements.

Answer: 2

### **Bootstrapping by hand**

To help you gain intuition about how bootstrapping works, imagine you have a data set that has only three points, `[-1, 0, 1]`. How many unique bootstrap samples can be drawn (e.g., `[-1, 0, 1]` and `[1, 0, -1]` are unique), and what is the maximum mean you can get from a bootstrap sample? It might be useful to jot down the samples on a piece of paper.

(These are too few data to get meaningful results from bootstrap procedures, but this example is useful for intuition.)

**Possible Answers**

1. There are 3 unique samples, and the maximum mean is 0.

2. There are 10 unique samples, and the maximum mean is 0.

3. There are 10 unique samples, and the maximum mean is 1.

4. There are 27 unique samples, and the maximum mean is 0.

5. There are 27 unique samples, and the maximum mean is 1.

Answer: 5

There are 27 total bootstrap samples, and one of them, [1,1,1] has a mean of 1. Conversely, 7 of them have a mean of zero.

### **Visualizing bootstrap samples**

In this exercise, you will generate bootstrap samples from the set of annual rainfall data measured at the Sheffield Weather Station in the UK from 1883 to 2015. The data are stored in the NumPy array `rainfall` in units of millimeters (mm). By graphically displaying the bootstrap samples with an ECDF, you can get a feel for how bootstrap sampling allows probabilistic descriptions of data.

**Instructions**

- Write a `for` loop to acquire `50` bootstrap samples of the rainfall data and plot their ECDF.
  - Use `np.random.choice()` to generate a bootstrap sample from the NumPy array `rainfall`. Be sure that the size of the resampled array is `len(rainfall)`.
  - Use the function `ecdf()` that you wrote in the prequel to this course to generate the `x` and `y` values for the ECDF of the bootstrap sample `bs_sample`.
  - Plot the ECDF values. Specify `color='gray'` (to make gray dots) and `alpha=0.1` (to make them semi-transparent, since we are overlaying so many) in addition to the `marker='.'` and `linestyle='none'` keyword arguments.
- Use `ecdf()` to generate `x` and `y` values for the ECDF of the original rainfall data available in the array `rainfall`.
- Plot the ECDF values of the original data.
- Hit 'Submit Answer' to visualize the samples!

```{python}
for sample in range(50):
    # Generate bootstrap sample: bs_sample
    bs_sample = np.random.choice(rainfall, size = len(rainfall))

    # Compute and plot ECDF from bootstrap sample
    x, y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker = '.', linestyle = 'none',
                 color = 'red')

# Compute and plot ECDF from original data
x, y = ecdf(rainfall)
_ = plt.plot(x, y, marker = '.', alpha = 0.1)

# Make margins and label axes
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')

# Show the plot
plt.show()
plt.clf()
```

Notice how the bootstrap samples give an idea of how the distribution of rainfalls is spread.

## **Bootstrap confidence intervals**

**1. Bootstrap confidence intervals**

In the last video, we learned how to take a set of data, create a bootstrap sample, and then compute a bootstrap replicate of a given statistic. Since we will repeat the replicates over and over again, we can write a function to generate a bootstrap replicate.

**2. Bootstrap replicate function**

We will call the function bootstrap_replicate_1d, since it works on one-dimensional arrays. We pass in the data and also a function that computes the statistic of interest. We could pass np dot mean or np dot median, for example. Generating a replicate takes two steps. First, we choose entries out of the data array so that the bootstrap sample has the same number of entries as the original data. Then, we compute the statistic using the specified function. If we call the function, we get a bootstrap replicate. And we can do this over and over again. So, how do we do it over and over again?

```{python}
def bootstrap_replicate_1d(data, func):
  """Generate bootstrap replicate of 1d data"""
  bs_sample = np.random.choice(data, len(data))
  return func(bs_sample)
bootstrap_replicate_1d(michelson_speed_of_light, np.mean)
```
```{python}
bootstrap_replicate_1d(michelson_speed_of_light, np.mean)
```
```{python}
bootstrap_replicate_1d(michelson_speed_of_light, np.mean)
```

**3. Many bootstrap replicates**

With a for loop! First, we have to initialize an array to store our bootstrap replicates. We will make 10,000 replicates, so we use np dot empty to create an empty array. Next, we write a for loop to generate a replicate and store it in the bs_replicates array. Now that we have the replicates,

```{python}
bs_replicates = np.empty(10000)
for i in range(10000):
  bs_replicates[i] = bootstrap_replicate_1d(michelson_speed_of_light, np.mean)
```
**4. Plotting a histogram of bootstrap replicates**

we can make a histogram to see what we might expect to get for the mean of repeated measurements of the speed of light. Note that we use the normed equals True keyword argument. This sets the height of the bars of the histogram such that the total area of the bars is equal to one. This is called

```{python}
_ = plt.hist(bs_replicates, bins = 30, density = True)
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()
plt.clf()
```
**5. Bootstrap estimate of the mean**

normalization, and we do it so that the histogram approximates a probability density function. You'll recall from the prequel to this course that the area under the PDF gives a probability. So, we have computed the approximate PDF of the mean speed of light we would expect to get if we performed the measurements again. Now we're thinking probabilistically! If we repeat the experiment again and again, we are likely to only see the sample mean vary by about 30 km/s. Now it is useful to summarize this result without having to resort to a graphical method like a histogram. To do this,

```{python}
_ = plt.hist(bs_replicates, bins = 30, density = True)
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()
plt.clf()
```

**6. Confidence interval of a statistic**

we will compute the 95% confidence interval of the mean. The p% confidence interval is defined as follows. If we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval. In our case, if we repeated the 100 measurements of the speed of light over and over again, 95% of the sample means would lie within the 95% confidence interval.

**7. Bootstrap confidence interval**

By doing bootstrap replicas, we just "repeated" the experiment over and over again. So, we just use np dot percentile to compute the 2-point-5th and 97-point-5th percentiles to get the 95% confidence interval. This is indeed commensurate with what we see in the histogram.

```{python}
conf_int = np.percentile(bs_replicates, [2.5, 97.5])
print(conf_int)
```

**8. Let's practice!**

Now it's time for you get some of your own bootstrap confidence intervals.

### **Generating many bootstrap replicates**

The function `bootstrap_replicate_1d()` from the video is available in your namespace. Now you'll write another function, `draw_bs_reps(data, func, size=1)`, which generates many bootstrap replicates from the data set. This function will come in handy for you again and again as you compute confidence intervals and later when you do hypothesis tests.

For your reference, the `bootstrap_replicate_1d()` function is provided below:

```{python, eval = FALSE}
def bootstrap_replicate_1d(data, func):
    """Generate bootstrap replicate of 1D data."""
    bs_sample = np.random.choice(data, len(data))
    return func(bs_sample)
```

**Instructions**

- Define a function with call signature `draw_bs_reps(data, func, size=1)`.
  - Using `np.empty()`, initialize an array called `bs_replicates` of size `size` to hold all of the bootstrap replicates.
   - Write a `for` loop that ranges over `size` and computes a replicate using `bootstrap_replicate_1d()`. Refer to the exercise description above to see the function signature of `bootstrap_replicate_1d()`. Store the replicate in the appropriate index of `bs_replicates`.
  - Return the array of replicates `bs_replicates`. This has already been done for you.

```{python}
def draw_bs_reps(data, func, size = 1):
  """Draw bootstrap replicates."""
  # Initialize array of replicates: bs_replicates
  bs_replicates = np.empty(size)
  # Generate replicates
  for i in range(size):
    bs_replicates[i] = bootstrap_replicate_1d(data, func)
  return bs_replicates
```

### **Bootstrap replicates of the mean and the SEM**

In this exercise, you will compute a bootstrap estimate of the probability density function of the mean annual rainfall at the Sheffield Weather Station. Remember, we are estimating the mean annual rainfall we would get if the Sheffield Weather Station could repeat all of the measurements from 1883 to 2015 over and over again. This is a *probabilistic* estimate of the mean. You will plot the PDF as a histogram, and you will see that it is Normal.

In fact, it can be shown theoretically that under not-too-restrictive conditions, the value of the mean will always be Normally distributed. (This does not hold in general, just for the mean and a few other statistics.) The standard deviation of this distribution, called the **standard error of the mean**, or SEM, is given by the standard deviation of the data divided by the square root of the number of data points. I.e., for a data set, `sem = np.std(data) / np.sqrt(len(data))`. Using hacker statistics, you get this same result without the need to derive it, but you will verify this result from your bootstrap replicates.

The dataset has been pre-loaded for you into an array called `rainfall`.

**Instructions**

- Draw `10000` bootstrap replicates of the mean annual rainfall using your `draw_bs_reps()` function and the `rainfall` array. Hint: Pass in `np.mean` for `func` to compute the mean.
  - As a reminder, `draw_bs_reps()` accepts 3 arguments: `data`, `func`, and `size`.
- Compute and print the standard error of the mean of `rainfall`.
  - The formula to compute this is `np.std(data) / np.sqrt(len(data))`.
- Compute and print the standard deviation of your bootstrap replicates `bs_replicates`.
- Make a histogram of the replicates using the `normed=True` keyword argument and 50 bins.
- Hit 'Submit Answer' to see the plot!

```{python}
# Take 10,000 bootstrap replicates of the mean: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.mean, 10000)

# Compute and print SEM
sem = np.std(rainfall) / np.sqrt(len(rainfall))
print(sem)

# Compute and print standard deviation of bootstrap replicates
bs_std = np.std(bs_replicates)
print(bs_std)

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins = 50, density = True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()
plt.clf()
```

Notice that the SEM we got from the known expression and the bootstrap replicates is the same and the distribution of the bootstrap replicates of the mean is Normal.

### **Confidence intervals of rainfall data**

A *confidence interval* gives upper and lower bounds on the range of parameter values you might expect to get if we repeat our measurements. For named distributions, you can compute them analytically or look them up, but one of the many beautiful properties of the bootstrap method is that you can take percentiles of your bootstrap replicates to get your confidence interval. Conveniently, you can use the `np.percentile()` function.

Use the bootstrap replicates you just generated to compute the 95% confidence interval. That is, give the 2.5th and 97.5th percentile of your bootstrap replicates stored as `bs_replicates`. What is the 95% confidence interval?

**Possible Answers**

1. (765, 776) mm/year

2. (780, 821) mm/year

3. (761, 817) mm/year

4. (761, 841) mm/year

```{python}
conf_int = np.percentile(bs_replicates, [2.5, 97.5])
print(conf_int)
```
Answer: 2

### **Bootstrap replicates of other statistics**

We saw in a previous exercise that the mean is Normally distributed. This does not necessarily hold for other statistics, but no worry: as hackers, we can always take bootstrap replicates! In this exercise, you'll generate bootstrap replicates for the variance of the annual rainfall at the Sheffield Weather Station and plot the histogram of the replicates.

Here, you will make use of the `draw_bs_reps()` function you defined a few exercises ago. It is provided below for your reference:

```{python}
def draw_bs_reps(data, func, size=1):
    """Draw bootstrap replicates."""
    # Initialize array of replicates
    bs_replicates = np.empty(size)
    # Generate replicates
    for i in range(size):
        bs_replicates[i] = bootstrap_replicate_1d(data, func)
    return bs_replicates
```

**Instructions**

- Draw `10000` bootstrap replicates of the variance in annual rainfall, stored in the `rainfall` dataset, using your `draw_bs_reps()` function. Hint: Pass in `np.var` for computing the variance.
- Divide your variance replicates (`bs_replicates`) by `100` to put the variance in units of square centimeters for convenience.
- Make a histogram of `bs_replicates` using the `normed=True` keyword argument and `50` bins.

```{python}
# Generate 10,000 bootstrap replicates of the variance: bs_replicates
bs_replicates = draw_bs_reps(rainfall, np.var, 10000)

# Put the variance in units of square centimeters
bs_replicates = bs_replicates/100

# Make a histogram of the results
_ = plt.hist(bs_replicates, bins = 50, density = True)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()
plt.clf()
```

This is not normally distributed, as it has a longer tail to the right. Note that you can also compute a confidence interval on the variance, or any other statistic, using np.percentile() with your bootstrap replicates.

### **Confidence interval on the rate of no-hitters**

Consider again the inter-no-hitter intervals for the modern era of baseball. Generate 10,000 bootstrap replicates of the optimal parameter $\tau$. Plot a histogram of your replicates and report a 95% confidence interval.

**Instructions**

- Generate `10000` bootstrap replicates of $\tau$ from the `nohitter_times` data using your `draw_bs_reps()` function. Recall that the optimal $\tau$ is calculated as the **mean** of the data.
- Compute the 95% confidence interval using `np.percentile()` and passing in two arguments: The array `bs_replicates`, and the list of percentiles - in this case `2.5` and `97.5`.
- Print the confidence interval.
- Plot a histogram of your bootstrap replicates. This has been done for you, so hit 'Submit Answer' to see the plot!

```{python}
# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins = 50, density = True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')

# Show the plot
plt.show()
plt.clf()

```

This gives you an estimate of what the typical time between no-hitters is. It could be anywhere between 643 and 895 games.

## **Pairs bootstrap**

**1. Pairs bootstrap**

When we computed bootstrap confidence intervals on summary statistics, we did so

**2. Nonparametric inference**

nonparametrically. By this, I mean that we did not assume any model underlying the data; the estimates were done using the data alone.

**3. 2008 US swing state election results**

When we performed a linear least squares regression, however, we were using a linear model, which has two parameters, the slope and intercept. This was a parametric estimate. The optimal parameter values we compute for our parametric model are like other statistics, in that we would get different values for them if we acquired the data again. We can perform bootstrap estimates to get confidence intervals on the slope and intercept as well. Remember: we need to think probabilistically. Let's consider the swing state election data from the prequel to this course. What if we had the election again, under identical conditions? How would the slope and intercept change? This is kind of a tricky question; there are several ways to get bootstrap estimates of the confidence intervals on these parameters, each of which makes difference assumptions about the data. We will do a method that makes the least assumptions,

```{python}
a, b = np.polyfit(election.total_votes, election.dem_share, 1)
x = np.array([0, 1000])
y = a * x*900 + b
g = sns.relplot(x = 'total', 
                y = 'dem_share', 
                data = election,
                kind = 'scatter',
                height = 2.6)
_ = g.set(xlabel = 'total votes (thousands)',
          ylabel = 'percent of vote for Obama',
          ylim = (0, 100),
          xlim = (0, 1000))
_ = plt.plot(x, y)
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**4. Pairs bootstrap for linear regression**

called pairs bootstrap. Since we cannot resample individual data because each county has two variables associated with it, the vote share for Obama and the total number of votes, we resample pairs. For the election data, we could randomly select a given county, and keep its total votes and Democratic share as a pair. So our bootstrap sample consists of a set (x,y) pairs. We then compute the slope and intercept from this pairs bootstrap sample to get the bootstrap replicates. You can get confidence intervals from many bootstrap replicates of the slope and intercept, just like before. Let's see how this works in practice.

**5. Generating a pairs bootstrap sample**

Because np dot random dot choice must sample a 1D array, we will sample the indices of the data points. We can generate the indices of a NumPy array using the np dot arrange function. It give us an array of sequential integers. We then sample the indices with replacement. The bootstrap sample is generated by slicing out the respective values from the original data arrays. With these in hand,

```{python}
np.arange(7)
```
```{python}
inds = np.arange(len(total))
bs_inds = np.random.choice(inds, len(inds))
bs_total_votes = total[bs_inds]
bs_dem_share = dem_share[bs_inds]
```

**6. Computing a pairs bootstrap replicate**

we can perform a linear regression using np dot polyfit on the pairs bootstrap sample to get a bootstrap replicate. If we compare the result to the linear regression on the original data, they are close, but not equal. As we have seen before, you can use many of these replicates to generate bootstrap confidence intervals for the slope and intercept using np dot percentile. You can also

```{python}
bs_slope, bs_intercept = np.polyfit(bs_total_votes, bs_dem_share, 1)
bs_slope, bs_intercept
```

```{python}
np.polyfit(total, dem_share, 1)
```


**7. 2008 US swing state election results**

plot the lines you get from your bootstrap replicates to get a graphic idea how the regression line may change if the data were collected again. You will work through this whole procedure in the exercises.

```{python}
def draw_bs_pairs_linreg(x, y, size=1):
  """Perform pairs bootstrap for linear regression."""
  # Set up array of indices to sample from: inds
  inds = np.arange(len(x))
  # Initialize replicates: bs_slope_reps, bs_intercept_reps
  bs_slope_reps = np.empty(size)
  bs_intercept_reps = np.empty(size)
  # Generate replicates
  for i in range(size):
    bs_inds = np.random.choice(inds, size = len(inds))
    bs_x, bs_y = x[bs_inds], y[bs_inds]
    bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
  return bs_slope_reps, bs_intercept_reps

# Generate replicates of slope and intercept using pairs bootstrap
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(total, dem_share, 1000)

# Generate array of x-values for bootstrap lines: x
x = np.array([0, max(total)])

# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, 
                 bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth = 0.5, alpha = 0.2, color = 'red')

# Plot the data
_ = plt.plot(total, dem_share, marker = '.', linestyle = 'none')

# Label axes, set the margins, and show the plot
_ = plt.xlabel('total votes (thousands)')
_ = plt.ylabel('percent of vote for Obama')
plt.margins(0.02)
plt.show()
plt.clf()
```
1 Data retrieved from Data.gov (https://www.data.gov/)

**8. Let's practice!**

When you do, always keep in mind that you are thinking probabilistically. Getting an optimal parameter value is the first step. Now, you are finding out how that parameter is likely to change upon repeated measurements. Happy coding!

### **A function to do pairs bootstrap**

As discussed in the video, pairs bootstrap involves resampling pairs of data. Each collection of pairs fit with a line, in this case using `np.polyfit()`. We do this again and again, getting bootstrap replicates of the parameter values. To have a useful tool for doing pairs bootstrap, you will write a function to perform pairs bootstrap on a set of `x,y` data.

**Instructions**

- Define a function with call signature `draw_bs_pairs_linreg(x, y, size=1)` to perform pairs bootstrap estimates on linear regression parameters.
   - Use `np.arange()` to set up an array of indices going from `0` to `len(x)`. These are what you will resample and use them to pick values out of the `x` and `y` arrays.
   - Use `np.empty()` to initialize the slope and intercept replicate arrays to be of size `size`.
  - Write a `for` loop to:
    - Resample the indices `inds`. Use `np.random.choice()` to do this.
    - Make new $x$ and $y$ arrays `bs_x` and `bs_y` using the the resampled indices `bs_inds`. To do this, slice x and `y` with `bs_inds`.
    - Use `np.polyfit()` on the new  and  arrays and store the computed slope and intercept.
- Return the pair bootstrap replicates of the slope and intercept.

```{python}
def draw_bs_pairs_linreg(x, y, size=1):
  """Perform pairs bootstrap for linear regression."""
  # Set up array of indices to sample from: inds
  inds = np.arange(len(x))
  # Initialize replicates: bs_slope_reps, bs_intercept_reps
  bs_slope_reps = np.empty(size)
  bs_intercept_reps = np.empty(size)
  # Generate replicates
  for i in range(size):
    bs_inds = np.random.choice(inds, size = len(inds))
    bs_x, bs_y = x[bs_inds], y[bs_inds]
    bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
  return bs_slope_reps, bs_intercept_reps
```

### **Pairs bootstrap of literacy/fertility data**

Using the function you just wrote, perform pairs bootstrap to plot a histogram describing the estimate of the slope from the illiteracy/fertility data. Also report the 95% confidence interval of the slope. The data is available to you in the NumPy arrays `illiteracy` and `fertility`.

As a reminder, `draw_bs_pairs_linreg()` has a function signature of `draw_bs_pairs_linreg(x, y, size=1)`, and it returns two values: `bs_slope_reps` and `bs_intercept_reps`.

**Instructions**

- Use your `draw_bs_pairs_linreg()` function to take `1000` bootstrap replicates of the slope and intercept. The x-axis data is `illiteracy` and y-axis data is `fertility`.
- Compute and print the 95% bootstrap confidence interval for the slope.
- Plot and show a histogram of the slope replicates. Be sure to label your axes. This has been done for you, so click 'Submit Answer' to see your histogram!

```{python}
# Generate replicates of slope and intercept using pairs bootstrap
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, 1000)

# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))

# Plot the histogram
_ = plt.hist(bs_slope_reps, bins = 50, density = True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')
plt.show()
plt.clf()
```

### **Plotting bootstrap regressions**

A nice way to visualize the variability we might expect in a linear regression is to plot the line you would get from each bootstrap replicate of the slope and intercept. Do this for the first 100 of your bootstrap replicates of the slope and intercept (stored as `bs_slope_reps` and `bs_intercept_reps`).

**Instructions**

- Generate an array of $x$-values consisting of `0` and `100` for the plot of the regression lines. Use the `np.array()` function for this.
- Write a `for` loop in which you plot a regression line with a slope and intercept given by the pairs bootstrap replicates. Do this for `100` lines.
  - When plotting the regression lines in each iteration of the `for` loop, recall the regression equation `y = a*x + b`. Here, `a` is `bs_slope_reps[i]` and `b` is `bs_intercept_reps[i]`.
   - Specify the keyword arguments `linewidth=0.5`, `alpha=0.2`, and `color='red'` in your call to `plt.plot()`.
- Make a scatter plot with `illiteracy` on the x-axis and `fertility` on the y-axis. Remember to specify the `marker='.'` and `linestyle='none'` keyword arguments.
- Label the axes, set a 2% margin, and show the plot. This has been done for you, so hit 'Submit Answer' to visualize the bootstrap regressions!

```{python}
# Generate array of x-values for bootstrap lines: x
x = np.array([0, 100])

# Plot the bootstrap lines
for i in range(100):
    _ = plt.plot(x, 
                 bs_slope_reps[i]*x + bs_intercept_reps[i],
                 linewidth = 0.5, alpha = 0.2, color = 'red')

# Plot the data
_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')

# Label axes, set the margins, and show the plot
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)
plt.show()
plt.clf()
```

You now have some serious chops for parameter estimation. Let's move on to hypothesis testing!