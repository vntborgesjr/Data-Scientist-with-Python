---
title: "01 - Preparing the data for analysis"
output: 
  html_notebook:
    toc: true
    toc_float: false
    toc_depth: 4
editor_options: 
  chunk_output_type: inline
---

```{python setup}
import pandas as pd
import numpy as np
```

## **Stanford Open Policing Project dataset**

**1. Stanford Open Policing Project dataset**

Hi, my name is Kevin Markham and I'll be your instructor for this course. I'm a data scientist and the founder of Data School. In this course, you'll be practicing a lot of what you've learned about pandas already to answer interesting questions about a real dataset. You'll gain valuable experience analyzing a dataset from start to finish, which will help to prepare you for your data science career.

**2. Introduction to the dataset**

Let's start by introducing the data. You'll be working with a dataset of traffic stops by police officers that was collected by the Stanford Open Policing Project. They've collected data from 31 US states, but in this course you'll be focusing on data from the state of Rhode Island. For size reasons, some of the columns and rows have been removed, but you can download the full dataset for any of the 31 states from the project's website.

**3. Preparing the data**

This first chapter is about preparing the data for analysis. Before beginning an analysis, it's critical that you first examine the data to make sure that you understand it, and then clean the data, to make working with it a more efficient process. As always, we'll start by importing pandas as pd. We'll use the read_csv() function to read in the dataset from a file, and then store it in a DataFrame called ri, which stands for Rhode Island. We'll use the head() method in order to take a quick glance at the DataFrame, though there are many more columns than can fit on this screen. Each row represents a single traffic stop. You'll notice that the county_name column contains NaN values, which indicate missing values. These are often values that were not collected during the data gathering process, or are irrelevant for that particular row.

**4. Locating missing values (1)**

It's important that you locate missing values so that you can proactively decide how to handle them. You may recall that the isnull() method generates a DataFrame of True and False values: True if the element is missing, and False if it's not.

**5. Locating missing values (2)**

One useful trick is to take the sum of this DataFrame, which outputs a count of the number of missing values in each column. How does that calculation work? Well, the sum() method calculates the sum of each column by default, and True values are treated as ones, while False values are treated as zeros.

**6. Dropping a column**

Let's compare these missing value counts to the DataFrame's shape. You'll notice that the county_name column contains as many missing values as there are rows, meaning that it only contains missing values. Since it contains no useful information, this column can be dropped using the drop() method. Besides specifying the column name, you need to specify that you're dropping from the columns axis, and that you want the operation to occur in place, which avoids an assignment statement.

**7. Dropping rows**

Finally, let's take a look at one more method related to missing values. The dropna() method is a great way to drop rows based on the presence of missing values in that row. For example, let's pretend that the stop_date and stop_time columns are critical to our analysis, and thus a row is useless to us without that data. We can tell pandas to drop all rows that have a missing value in either the stop_date or stop_time column. Because we specified a subset, the dropna() method only takes these two columns into account when deciding which rows to drop.

**8. Let's practice!**

Now it's your turn to practice using these functions to examine and clean this dataset.

### **Examining the dataset**

Throughout this course, you'll be analyzing a dataset of traffic stops in Rhode Island that was collected by the [Stanford Open Policing Project](https://openpolicing.stanford.edu/).

Before beginning your analysis, it's important that you familiarize yourself with the dataset. In this exercise, you'll read the dataset into pandas, examine the first few rows, and then count the number of missing values.

**Instructions**

- Import `pandas` using the alias `pd`.
- Read the file `police.csv` into a DataFrame named `ri`.
- Examine the first 5 rows of the DataFrame (known as the "head").
- Count the number of missing values in each column: Use `.isnull()` to check which DataFrame elements are missing, and then take the `.sum()` to count the number of `True` values in each column.

```{python}
# Import the pandas library as pd
import ____ as ____

# Read 'police.csv' into a DataFrame named ri
ri = pd.____(____)

# Examine the head of the DataFrame
print(ri.____)

# Count the number of missing values in each column
print(ri.isnull().____)
```

### **Dropping columns**

Often, a DataFrame will contain columns that are not useful to your analysis. Such columns should be dropped from the DataFrame, to make it easier for you to focus on the remaining columns.

In this exercise, you'll drop the `county_name` column because it only contains missing values, and you'll drop the `state` column because all of the traffic stops took place in one state (Rhode Island). Thus, these columns can be dropped because they contain no useful information. The number of missing values in each column has been printed to the console for you.

**Instructions**

- Examine the DataFrame's `.shape` to find out the number of rows and columns.
- Drop both the `county_name` and `state` columns by passing the column names to the `.drop()` method as a list of strings.
- Examine the `.shape` again to verify that there are now two fewer columns.

```{python}
# Examine the shape of the DataFrame
print(ri.____)

# Drop the 'county_name' and 'state' columns
ri.____([____, ____], axis='columns', inplace=True)

# Examine the shape of the DataFrame (again)
print(____)
```

### **Dropping rows**

When you know that a specific column will be critical to your analysis, and only a small fraction of rows are missing a value in that column, it often makes sense to remove those rows from the dataset.

During this course, the `driver_gender` column will be critical to many of your analyses. Because only a small fraction of rows are missing `driver_gender`, we'll drop those rows from the dataset.

**Instructions**

- Count the number of missing values in each column.
- Drop all rows that are missing `driver_gender` by passing the column name to the `subset` parameter of `.dropna()`.
- Count the number of missing values in each column again, to verify that none of the remaining rows are missing `driver_gender`.
- Examine the DataFrame's `.shape` to see how many rows and columns remain.

```{python}
# Count the number of missing values in each column
print(ri.isnull().____)

# Drop all rows that are missing 'driver_gender'
ri.____(subset=[____], inplace=True)

# Count the number of missing values in each column (again)
print(ri.____.____)

# Examine the shape of the DataFrame
print(____)
```

## **Using proper data types**

**1. Using proper data types**

In the last set of exercises, you began cleaning the dataset by removing columns and rows that will not be useful for your upcoming analyses. Now, we're going to continue cleaning the dataset by ensuring that each of the columns has the proper data type.

**2. Examining the data types**

Let's take a look at the dtypes attribute of the DataFrame. Every Series has a data type, which was automatically inferred by pandas when it was reading in the CSV file. As you can see, the only data types currently in use are object and bool. The object data type usually means that a Series is made up of Python strings, though it can indicate the presence of other Python objects such as lists. The bool data type is short for Boolean, which means that a Series is made up of True and False values. pandas also supports other data types, such as int for integers, float for floating point values, datetime for dates and times, and category for categorical variables.

**3. Why do data types matter?**

But why does the data type of a pandas Series even matter? Data types matter mostly because they affect which operations you can perform on a given Series. In particular, it's beneficial not to store data as strings when possible. For example, mathematical operations can be performed on ints and floats, but those operations will fail if the numbers are stored as strings. The datetime type enables a rich set of date-based attributes and methods that are not possible with strings. The category data type results in less memory usage and faster processing than strings. And the bool data type enables logical and mathematical operations that we'll use during the course.

**4. Fixing a data type**

Let's see an example of how you might fix an improper data type. We'll imagine a DataFrame named apple that has a Series named price, which stores the closing price of Apple company stock each day. You can check the data type of the price Series using its dtype attribute. It reports a dtype of "O", which stands for object and means that the numbers are actually stored as strings. To change the data type of the price Series from object to float, you can use the astype() method, to which you pass the new data type as an argument. Then, you simply overwrite the original Series. If you check the data type again, you can see that it has changed to float. You might have noticed that on the right side of the equals sign, I used dot notation to refer to the price Series, rather than bracket notation. They mean the same thing, but I'll be using dot notation throughout this course, because I find that dot notation makes pandas code more readable. However, it's worth noting that you must use bracket notation on the left side of an assignment statement to create a new Series or overwrite an existing Series.

**5. Let's practice!**

It's now time for you to practice examining and fixing data types in our dataset.

### **Finding an incorrect data type**

The `dtypes` attribute of the `ri` DataFrame has been printed for you. Your task is to explore the `ri` DataFrame in the IPython Shell to determine which column's data type should be changed.

**Possible Answers**

1. `stop_time` should have a data type of `float`

2. `search_conducted` should have a data type of `object`

3. `is_arrested` should have a data type of `bool`

4. `district` should have a data type of `int`

Answer:

### **Fixing a data type**

We saw in the previous exercise that the `is_arrested` column currently has the object data type. In this exercise, we'll change the data type to `bool`, which is the most suitable type for a column containing True and False values.

Fixing the data type will enable us to use mathematical operations on the `is_arrested` column that would not be possible otherwise.

**Instructions**

- Examine the head of the `is_arrested` column to verify that it contains `True` and `False` values and to check the column's data type.
- Use the `.astype()` method to convert `is_arrested` to a `bool` column.
- Check the new data type of `is_arrested` to confirm that it is now a `bool` column.

```{python}
# Examine the head of the 'is_arrested' column
print(ri.is_arrested.____)

# Change the data type of 'is_arrested' to 'bool'
ri['is_arrested'] = ri.is_arrested.____

# Check the data type of 'is_arrested' 
print(____)
```

## **Creating a DatetimeIndex**

**1. Creating a DatetimeIndex**

In the last exercise, you fixed the data type of the is_arrested column. Now, we're going to build a DatetimeIndex for our DataFrame.

**2. Using datetime format**

Let's take a look at the head of the dataset again. As you can see, the date and time of each traffic stop are stored in separate columns, both of which are object columns. Because we'll be using stop_date and stop_time in our analysis, we're going to combine these two columns into a single column and then convert it to pandas' datetime format. This will be beneficial because unlike object columns, datetime columns provide date-based attributes that will make our analysis easier.

**3. Combining object columns**

Let's see an example of this using the apple stock price DataFrame from the previous video. Date and time are stored in separate columns, so the first task is to combine these two columns using a string method. As you might remember from previous courses, string methods, such as replace(), are Series methods available via the str accessor. In this example, we're replacing the forward slash in the date column with a dash. It outputs a new Series in which the string replacement has been made, though this change is temporary since we haven't saved the new Series. Anyway, to combine the columns, we're going to use the str dot cat() method, which is short for concatenate. We'll concatenate the date column with the time column, and tell pandas to separate them with a space, storing the result in a Series object named combined. You can see that the combined Series contains both the date and time. It's still an object column, but it's now ready for conversion to datetime format.

**4. Converting to datetime format**

To convert the combined Series to datetime format, you simply pass it to the to_datetime() function, and store the result in a new column. We didn't even need to specify that the original data was in month-day-year format, instead pandas just figured it out. Looking at the updated DataFrame, you can see that the new column contains both the date and time, and that it is stored in a more standard way. From the dtypes attribute, you can see that the new data type of the new column is datetime, instead of object.

**5. Setting the index**

One final step that we'll take is to set the datetime column as the index. That will make it easier to filter the DataFrame by date, plot the data by date, and so on. We'll use the set_index() method, and specify that the operation should occur in place to avoid an assignment statement. You can see that the default index has been replaced with the datetime column. And the index is now a special type called DatetimeIndex. As a reminder, when an existing column becomes the index, it is no longer considered to be one of the DataFrame columns.

**6. Let's practice!**

Now that you've seen how to create a DatetimeIndex for the apple DataFrame, you can practice these steps on our dataset of traffic stops. This is the final step before we begin our analysis of the dataset in the next chapter.

### **Combining object columns**

Currently, the date and time of each traffic stop are stored in separate object columns: `stop_date` and `stop_time`.

In this exercise, you'll combine these two columns into a single column, and then convert it to datetime format. This will enable convenient date-based attributes that we'll use later in the course.

**Instructions**

- Use a string method to concatenate `stop_date` and `stop_time` (separated by a space), and store the result in `combined`.
- Convert `combined` to `datetime` format, and store the result in a new column named `stop_datetime`.
- Examine the DataFrame `.dtypes` to confirm that `stop_datetime` is a `datetime` column.

```{python}
# Concatenate 'stop_date' and 'stop_time' (separated by a space)
combined = ri.stop_date.____

# Convert 'combined' to datetime format
ri['stop_datetime'] = ____

# Examine the data types of the DataFrame
print(____)
```

### **Setting the index**

The last step that you'll take in this chapter is to set the `stop_datetime` column as the DataFrame's index. By replacing the default index with a `DatetimeIndex`, you'll make it easier to analyze the dataset by date and time, which will come in handy later in the course!

**Instructions**

- Set `stop_datetime` as the DataFrame index.
- Examine the index to verify that it is a `DatetimeIndex`.
- Examine the DataFrame columns to confirm that `stop_datetime` is no longer one of the columns.

```{python}
# Set 'stop_datetime' as the index
ri.____(____, inplace=True)

# Examine the index
print(____)

# Examine the columns
print(____)
```

