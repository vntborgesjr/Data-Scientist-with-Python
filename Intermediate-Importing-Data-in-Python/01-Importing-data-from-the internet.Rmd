---
title: "01 -Importing data from the internet"
output:
  html_notebook:
    toc: yes
    toc_float: no
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: '4'
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{python setup}
import pandas as pd
import numpy as np
#import BeautifulSoup
import urllib
import requests
```

## **Importing flat files from the web**

**1. Importing flat files from the web**

You're now able to import data in Python from all sorts of file types:

**2. You’re already great at importing!**

flat files such as dot txt's and dot csv's, other file types such as pickled files, Excel spreadsheets and MATLAB files. You've also gained valuable experience in querying relational databases to import data from them using SQL. You have really come a long way, congratulations! However, all of these skills involve importing data from files that you have locally. Much of the time as a data scientist, these skills won't be quite enough because you won't always have the data that you need. You will need to import it from the web.

**3. Can you import web data?**

Say, for example, you want to import the Wine Quality dataset from the Machine Learning Repository hosted by the University of California, Irvine. How do you get this file from the web? Now you could use your favourite web browser of choice to navigate to the relevant URL, point and click on the appropriate hyperlinks to download the file but this poses a few problems. Firstly, it isn't written in code and so poses reproducibility problems. If another Data Scientist wanted to reproduce your workflow it, she would necessarily have to do so outside Python. Secondly, it is NOT scalable. If you wanted to download one hundred or one thousand such files, it would take one hundred or one thousand times as long, respectively, whereas if you wrote it in code, your workflow could scale.

**4. You’ll learn how to…**

As reproducibility and scalability are situated at the very heart of Data Science, you're going to learn in this chapter how to use Python code to import and locally save datasets from the world wide web. You'll also learn how to load such datasets into pandas dataframes directly from the web, whether they be flat files or otherwise. Then you'll place these skills in the wider context of making HTTP requests. In particular, you'll make HTTP GET requests, which in plain English means getting data from the web. You'll use these new Request skills to learn the basics of scraping HTML from the internet and you'll use the wonderful Python package BeautifulSoup to parse the HTML and turn it into data. There are a number of great packages to help us import web data: herein, you'll become familiar with the urllib and requests packages. We'll first check out urllib:

**5. The urllib package**

"This module provides a high-level interface for fetching data across the World Wide Web. In particular, the urlopen function is similar to the built-in function open, but accepts Universal Resource Locators (URLs) instead of filenames." Let's now dive directly in to importing data from the web with an example, importing the Wine Quality dataset for white wine. Don't get jealous: in the first interactive exercise, it will be your job to import the red wine dataset!

**6. How to automate file download in Python**

All we have done here is imported a function called urlretrieve from the request subpackage of the urllib package, we assigned the relevant URL as a string to the variable url. We then used the urlretrieve function to write the contents of the url to a file 'winequality-white dot csv'. Now it's your turn to do the same but for red wine!

```{python}
from urllib.request import rulretrieve
url = 'http://archive.ics.uci.edu/machine-learning-database/wine-quality/winequality-white.csv'
urlretrieve(url, 'winequality-white.csv')
```

**7. Let's practice!**

In the following interactive exercises you'll also figure out how to use pandas to load the contents of web files directly into pandas dataframes without first having to save them locally. Happy hacking!

### **Importing flat files from the web: your turn!**

You are about to import your first file from the web! The flat file you will import will be `'winequality-red.csv'` from the University of California, Irvine's [Machine Learning](http://archive.ics.uci.edu/ml/index.html) repository. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.

The URL of the file is

`'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'`

After you import it, you'll check your working directory to confirm that it is there and then you'll load it into a `pandas` DataFrame.

**Instructions**

- Import the function `urlretrieve` from the subpackage `urllib.request`.
- Assign the URL of the file to the variable `url`.
- Use the function `urlretrieve()` to save the file locally as `'winequality-red.csv'`.
- Execute the remaining code to load `'winequality-red.csv'` in a pandas DataFrame and to print its head to the shell.

```{python}
# Import package
from urllib.request import urlretrieve

# Import pandas
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Save file locally
urlretrieve(url, 'winequality-red.csv')

# Read file into a DataFrame and print its head
df = pd.read_csv('winequality-red.csv', sep = ';')
print(df.head())
```

### **Opening and reading flat files from the web**

You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using `pandas`. In particular, you can use the function `pd.read_csv()` with the URL as the first argument and the separator `sep` as the second argument.

The URL of the file, once again, is

`'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'`

**Instructions**

- Assign the URL of the file to the variable `url`.
- Read file into a DataFrame `df` using `pd.read_csv()`, recalling that the separator in the file is `';'`.
- Print the head of the DataFrame `df`.
- Execute the rest of the code to plot histogram of the first feature in the DataFrame `df`.

```{python}
# Import packages
import matplotlib.pyplot as plt
import pandas as pd

# Assign url of file: url
url = 'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'

# Read file into a DataFrame: df
df = pd.read_csv(url, sep = ';')

# Print the head of the DataFrame
print(df.head())

# Plot first column of df
_ = plt.hist(df['fixed acidity'])
_ = plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')
_ = plt.ylabel('count')
plt.show()
plt.clf()
```

### **Importing non-flat files from the web**

Congrats! You've just loaded a flat file from the web into a DataFrame without first saving it locally using the `pandas` function `pd.read_csv()`. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you'll use `pd.read_excel()` to import an Excel spreadsheet.

The URL of the spreadsheet is

`'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'`

Your job is to use `pd.read_excel()` to read in all of its sheets, print the sheet names and then print the head of the first sheet *using its name, not its index*.

Note that the output of `pd.read_excel()` is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.

**Instructions**

- Assign the URL of the file to the variable `url`.
- Read the file in `url` into a dictionary `xls` using `pd.read_excel()` recalling that, in order to import all sheets you need to pass `None` to the argument `sheet_name`.
- Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary `xls`.
Print the head of the first sheet *using the sheet name, not the index of the sheet*! The sheet name is `'1700'`

```{python}
# Import package
import pandas as pd

# Assign url of file: url
url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'

# Read in all sheets of Excel file: xls
xls = pd.read_excel(url, sheet_name = None)

# Print the sheetnames to the shell
print(xls.keys())

# Print the head of the first sheet (using its name, NOT its index)
print(xls['1700'].head())

```

## **HTTP request to import files from the web**

**1. HTTP requests to import files from the web**

Congrats on importing your first web data! In order to import files from the web,

**2. URL**

we used the urlretrieve function from urllib dot requests. Lets now unpack this a bit and, in the process, understand a few things about how the internet works. URL stands for Uniform or Universal Resource Locator and all they really are are references to web resources. The vast majority of URLs are web addresses, but they can refer to a few other things, such as file transfer protocols (FTP) and database access. We'll currently focus on those URLs that are web addresses OR the locations of websites. Such a URL consists of 2 parts, a protocol identifier http or https and a resource name such as datacamp dot com. The combination of protocol identifier and resource name uniquely specifies the web address! To explain URLs, I have introduced yet another acronym

**3. HTTP**

http, which itself stands for HyperText Transfer Protocol. Wikipedia provides a great description of HTTP. "The Hypertext Transfer Protocol (HTTP) is an application protocol for distributed, collaborative, hypermedia information systems. HTTP is the foundation of data communication for the World Wide Web." Note that HTTPS is a more secure form of HTTP. Each time you go to a website, you are actually sending an HTTP request to a server. This request is known as a GET request, by far the most common type of HTTP request. We are actually performing a GET request when using the function urlretrieve. The ingenuity of urlretrieve also lies in fact that it not only makes a GET request but also saves the relevant data locally. In the following, you'll learn how to make more GET requests to store web data in your environment. In particular, you'll figure out how to get the HTML data from a webpage. HTML stands for Hypertext Markup Language and is the standard markup language for the web.

**4. GET requests using urllib**

To extract the html from the wikipedia home page, you import the necessary functions, specify the URL, package the GET request using the function Request, send the request and catch the response using the function urlopen. This returns an HTTPResponse object, which has an associated read method. You then apply this read method to the response, which returns the HTML as a string, which you store in the variable html. You remember to be polite and close the response!

```{python}
from urllib.request import urlopen, Request
url = 'https://www.wikipedia.org/'
request = Request(url)
response = urlopen(request)
html = response.read()
response.close()
```

**5. GET requests using requests**

Now we are going to do the same, however here we'll use the requests package, which provides a wonderful API for making requests. According to the requests package website. "Requests allows you to send organic, grass-fed HTTP/1 dot 1 requests, without the need for manual labor." and the following organizations claim to use requests internally: "Her Majesty's Government, Amazon, Google, Twilio, NPR, Obama for America, Twitter, Sony, and Federal U.S. Institutions that prefer to be unnamed."

**6. GET requests using requests**

Moreover, "Requests is one of the most downloaded Python packages of all time, pulling in over 7,000,000 downloads every month. All the cool kids are doing it!" Lets now see requests at work. Here, you import the package requests, specify the URL, package the request, send the request and catch the response with a single function requests dot get; apply the text method to the response which returns the HTML as a string.

**7. Let's practice!**

That's enough out of me for the time being. Let's get you hacking away at pulling down some HTML from the web using GET requests! GET coding!

### **Performing HTTP requests in Python using urllib**

Now that you know the basics behind HTTP GET requests, it's time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, `"https://campus.datacamp.com/courses/1606/4135?ex=2"`.

In the next exercise, you'll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.

**Instructions**

- Import the functions `urlopen` and `Request` from the subpackage `urllib.request`.
- Package the request to the `url` `"https://campus.datacamp.com/courses/1606/4135?ex=2"` using the function `Request()` and assign it to `request`.
- Send the request and catch the response in the variable `response` with the function `urlopen()`.
- Run the rest of the code to see the datatype of `response` and to close the connection!

```{python}
# Import packages


# Specify the url
url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request: request


# Sends the request and catches the response: response


# Print the datatype of response
print(type(response))

# Be polite and close the response!
response.close()

```

### **Printing HTTP request results in Python using urllib**

You have just packaged and sent a GET request to `"https://campus.datacamp.com/courses/1606/4135?ex=2"` and then caught the response. You saw that such a response is a `http.client.HTTPResponse` object. The question remains: what can you do with this response?

Well, as it came from an HTML page, you could read it to extract the HTML and, in fact, such a `http.client.HTTPResponse` object has an associated `read()` method. In this exercise, you'll build on your previous great work to extract the response and print the HTML.

**Instructions**

- Send the request and catch the response in the variable response with the function `urlopen()`, as in the previous exercise.
- Extract the response using the `read()` method and store the result in the variable `html`.
- Print the string `html`.
- Hit submit to perform all of the above and to close the response: be tidy!

```{python}
# Import packages
from urllib.request import urlopen, Request

# Specify the url
url = "https://campus.datacamp.com/courses/1606/4135?ex=2"

# This packages the request
request = Request(url)

# Sends the request and catches the response: response


# Extract the response: html


# Print the html


# Be polite and close the response!
response.close()
```

### **Performing HTTP requests in Python using requests

Now that you've got your head and hands around making HTTP requests using the urllib package, you're going to figure out how to do the same using the higher-level requests library. You'll once again be pinging DataCamp servers for their `"http://www.datacamp.com/teach/documentation"` page.

Note that unlike in the previous exercises using urllib, you don't have to close the connection when using requests!

**Instructions**

- Import the package `requests`.
- Assign the URL of interest to the variable `url`.
- Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.
- Use the `text` attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `text`.
- Hit submit to print the HTML of the webpage.

```{python}
# Import package


# Specify the url: url


# Packages the request, send the request and catch the response: r


# Extract the response: text


# Print the html
print(text)
```

## **Scraping the web in Python**

**1. Scraping the web in Python**

Wow! you have just scraped HTML data from the web and you've done so using two different packages, urllib and requests. You also saw that requests provided a higher-level interface in that you needed to write less lines of to retrieve the relevant HTML as a string.

**2. HTML**

You've got the HTML of your page of interest but, generally HTML is a humble-jumble mix of both unstructured and structured data. A word on these terms: Structured data is data that has a pre-defined data model or that is organized in a defined manner. Unstructured data is data that does not possess either of these properties. HTML is interesting because, although much of it is unstructured text, it does contain tags that determine where, for examples, headings can be found, and hyperlinks.

**3. BeautifulSoup**

In general, to turn HTML that you have scraped from the world wide web into useful data, you'll need to parse it and extract structured data from it. In this video and the next few interactive exercises, we'll provide a brief introduction to how you can perform such tasks using the Python package BeautifulSoup. Lets check out the package's website. The first words at the top are: "You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects." Firstly, a word on the name of the package: BeautifulSoup? In web development, the term "tag soup" refers to structurally or syntactically incorrect HTML code written for a web page. What Beautiful Soup does best is to make tag soup beautiful again and to extract information from it with ease! In fact, the main object created and queried when using this package is called BeautifulSoup and it has a very important associated method called prettify! Lets now see BeautifulSoup in Beautiful Action!

**4. BeautifulSoup**

Once again, you use requests to scrape the HTML from the web. Then you create a BeautifulSoup object from the resulting HTML and prettify.

**5. Prettified Soup**

Printing the prettified Soup and the original HTML, you can see that for, example, the prettified Soup is indented in the way you would expect properly written HTML to be.

**6. Exploring BeautifulSoup**

You'll explore a few of the methods that you can apply to your soupified HTML in the following exercises, such as title and get_text, which extract the title and text, respectively.

**7. Exploring BeautifulSoup**

You'll also work with the Soupy method find_all in order to extract the URLs of all of the hyperlinks in the HTML. These are merely a few of many methods existing in BeautifulSoup to extract data from HTML. If, after completing these exercises, you find yourself thirsting for more BeautifulSoup, there are plenty of great resources on their website.

**8. Let's practice!**

Okay, now it's your turn to jump into the deep end of the proverbial soup bowl! Happy hacking!

### **Parsing HTML with BeautifulSoup**

In this interactive exercise, you'll learn how to use the BeautifulSoup package to `parse`, `prettify` and `extract` information from HTML. You'll scrape the data from the webpage of Guido van Rossum, Python's very own [Benevolent Dictator for Life](https://en.wikipedia.org/wiki/Benevolent_dictator_for_life). In the following exercises, you'll prettify the HTML and then extract the text and the hyperlinks.

The URL of interest is `url = 'https://www.python.org/~guido/'`.

**Instructions**

- Import the function `BeautifulSoup` from the package `bs4`.
- Assign the URL of interest to the variable `url`.
- Package the request to the URL, send the request and catch the response with a single function `requests.get()`, assigning the response to the variable `r`.
- Use the `text` attribute of the object `r` to return the HTML of the webpage as a string; store the result in a variable `html_doc`.
- Create a BeautifulSoup object `soup` from the resulting HTML using the function `BeautifulSoup()`.
- Use the method `prettify()` on `soup` and assign the result to `pretty_soup`.
- Hit submit to print to prettified HTML to your shell!

```{python}
# Import packages
import requests
from ____ import ____

# Specify url: url


# Package the request, send the request and catch the response: r


# Extracts the response as html: html_doc


# Create a BeautifulSoup object from the HTML: soup


# Prettify the BeautifulSoup object: pretty_soup


# Print the response
print(pretty_soup)
```

### **Turning a webpage into data using BeautifulSoup: getting the text**

As promised, in the following exercises, you'll learn the basics of extracting information from HTML soup. In this exercise, you'll figure out how to extract the text from the BDFL's webpage, along with printing the webpage's title.

**Instructions**

- In the sample code, the HTML response object `html_doc` has already been created: your first task is to Soupify it using the function `BeautifulSoup()` and to assign the resulting soup to the variable `soup`.
- Extract the title from the HTML soup `soup` using the attribute `title` and assign the result to `guido_title`.
- Print the title of Guido's webpage to the shell using the `print()` function.
- Extract the text from the HTML soup `soup` using the method `get_text()` and assign to `guido_text`.
- Hit submit to print the text from Guido's webpage to the shell.

```{python}
# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url: url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extract the response as html: html_doc
html_doc = r.text

# Create a BeautifulSoup object from the HTML: soup


# Get the title of Guido's webpage: guido_title


# Print the title of Guido's webpage to the shell


# Get Guido's text: guido_text


# Print Guido's text to the shell
print(guido_text)
```

### **Turning a webpage into data using BeautifulSoup: getting the hyperlinks**

In this exercise, you'll figure out how to extract the URLs of the hyperlinks from the BDFL's webpage. In the process, you'll become close friends with the soup method `find_all()`.

**Instructions**

- Use the method `find_all()` to find all hyperlinks in `soup`, remembering that hyperlinks are defined by the HTML tag `<a>` but passed to `find_all()` without angle brackets; store the result in the variable `a_tags`.
- The variable `a_tags` is a results set: your job now is to enumerate over it, using a `for` loop and to print the actual URLs of the hyperlinks; to do this, for every element `link` in `a_tags`, you want to `print()` `link.get('href')`.

```{python}
# Import packages
import requests
from bs4 import BeautifulSoup

# Specify url
url = 'https://www.python.org/~guido/'

# Package the request, send the request and catch the response: r
r = requests.get(url)

# Extracts the response as html: html_doc
html_doc = r.text

# create a BeautifulSoup object from the HTML: soup
soup = BeautifulSoup(html_doc)

# Print the title of Guido's webpage
print(soup.title)

# Find all 'a' tags (which define hyperlinks): a_tags


# Print the URLs to the shell
for ____ in ____:
    ____
```

